{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnQDRvXZFPJr"
      },
      "source": [
        "Make sure you fill your name and NetID below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0dj1tSpFPJt"
      },
      "outputs": [],
      "source": [
        "NAME = \"Vikram Sahai Saxena\"\n",
        "NET_ID = \"vs799\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2SN6uNmFPJu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFvJRejWFPJv"
      },
      "source": [
        "---\n",
        "# Assignment 3: RNN, LSTM, Attention, and Transformers\n",
        "\n",
        "In this assignment, you will implement neural machine translation (NMT) models using:\n",
        "\n",
        "1. RNNs\n",
        "2. LSTMs and LSTMs with attention\n",
        "3. Transformers.\n",
        "\n",
        "As in the previous assignments, you will see code blocks that look like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "pass\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n",
        "```python\n",
        "###############################################################################\n",
        "# TODO: Create a variable x with value 3.7\n",
        "###############################################################################\n",
        "x = 3.7\n",
        "# END OF YOUR CODE\n",
        "```\n",
        "\n",
        "Also, please remember:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPt-g1VfFPJx"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First let's import some libraries that will be useful in this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDCQUVq2FPJy"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzhxm48QFPJz"
      },
      "source": [
        "Make sure you are using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iDjkpA51FPJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c26ed28-4740-4a41-d32c-8b03d595ddf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('Good to go!')\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  print('Using CPU.')\n",
        "  device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fqOr31rFPJ1"
      },
      "source": [
        "For this assignment, we will use an English-to-French dataset. As shown below, the dataset contains multiple lines each of which has an English sentence and its French translation separated by a tab. In this problem, since English is translated to French, English is the source language and French is the target language. Note that each text sequence is of variable lengnth and can be just one sentence or a paragraph of multiple sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fAVGoNqSFPJ2"
      },
      "outputs": [],
      "source": [
        "def download_if_not_exist(file_name):\n",
        "\n",
        "  if not os.path.exists(file_name):\n",
        "    import urllib.request\n",
        "    DATA_URL = 'https://download.pytorch.org/tutorial/data.zip'\n",
        "\n",
        "    file_name, _ = urllib.request.urlretrieve(DATA_URL, './data.zip')\n",
        "\n",
        "  return file_name\n",
        "\n",
        "def read_raw(file_name):\n",
        "  file_name = download_if_not_exist(file_name)\n",
        "\n",
        "  with zipfile.ZipFile(file_name, 'r') as fzip:\n",
        "    raw_text = fzip.read(file_name.split('.')[-2][1:] + '/eng-fra.txt').decode('utf-8')\n",
        "  return raw_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hQWj8nELFPJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49da822b-9c83-457c-f72a-8036d6071ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tVa !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'aide !\n",
            "Jump.\tSaute.\n",
            "Stop!\tÇa suffit !\n",
            "Stop!\tStop !\n",
            "Stop!\tArrête-toi !\n",
            "Wait!\tAttends !\n",
            "Wait!\tAttendez !\n",
            "I see.\tJe comprends.\n"
          ]
        }
      ],
      "source": [
        "raw_text = read_raw('./data.zip')\n",
        "print(raw_text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51PBrJ_0FPJ3"
      },
      "source": [
        "Next we'll do some preprocessing on this raw text. We need to replace special symbols (non-breaking spaces) with spaces, convert all characters to lower case, and insert a space between words and punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F1WJAUQlFPJ4"
      },
      "outputs": [],
      "source": [
        "def preprocess_raw(text):\n",
        "  text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
        "  out = ''\n",
        "  for i, char in enumerate(text.lower()):\n",
        "    if char in (',', '!', '.') and i > 0 and text[i-1] != ' ':\n",
        "      out += ' '\n",
        "    out += char\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AvchKQ2FPJ4"
      },
      "source": [
        "We further split the source-target pairs into a source list and a target list. We use word-level tokenization here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LATaw74aFPJ4"
      },
      "outputs": [],
      "source": [
        "def split_source_target(text, max_len):\n",
        "  source, target = [], []\n",
        "  for i, line in enumerate(text.split('\\n')):\n",
        "    if i > 5000: # we only use 5000 pairs of translation\n",
        "      break\n",
        "    parts = line.split('\\t')\n",
        "    if len(parts) == 2:\n",
        "      src_tokens = parts[0].split(' ')\n",
        "      tgt_tokens = parts[1].split(' ')\n",
        "      if (len(src_tokens) <= max_len) and (len(tgt_tokens) <= max_len):\n",
        "        source.append(src_tokens)\n",
        "        target.append(tgt_tokens)\n",
        "  return source, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ksrZ0MC_FPJ6"
      },
      "outputs": [],
      "source": [
        "def prepare_data(raw_text, max_len=10000):\n",
        "  text = preprocess_raw(raw_text)\n",
        "  source, target = split_source_target(text, max_len)\n",
        "  return source, target\n",
        "\n",
        "source, target = prepare_data(raw_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8131HkNWFPJ6"
      },
      "source": [
        "Using the whole dataset takes too much memory, and it is hard to train with a large vocabulary. Thus, we will filter out some words by looking at the statistical properties of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nMM2lENjFPJ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "892147b3-46b1-4cbe-9914-2318fd3cb3a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdRklEQVR4nO3dd1hTZ/8G8DsECHsoyFBExIEbJ8VRF0qtYn2tddRdR+tr66ptHT+1rQO1VXHW2qFdvmrV2tatiFpH69a2bgXFwbAqWwLJ8/sjTSQyJBA4JLk/18XF4eTkOd9EhJvzjCMTQggQERERScRK6gKIiIjIsjGMEBERkaQYRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBCREREkmIYISIiIkkxjJAkhg0bhho1ahi1zXXr1kEmkyEuLs6o7ZaFDz/8EDKZrFzO1aFDB3To0EH39cGDByGTybB58+ZyOX9Z/FsbW3p6OkaOHAlvb2/IZDJMmDBB6pKoAouLi4NMJsOnn34qdSlmg2HEhN24cQNvvvkmatasCTs7O7i4uKBNmzZYunQpsrKypC6vzMybNw/btm2TugwdbQjSftjZ2cHX1xfh4eFYtmwZ0tLSjHKee/fu4cMPP8S5c+eM0p4xVeTaimPevHlYt24dxowZg++++w6DBw8u9FilUomlS5eiadOmcHFxgZubGxo0aIDRo0fj8uXLZVrn+vXrERUVVabnKE8dOnRAw4YNpS6jUDt37sSHH34odRmWQZBJ2r59u7C3txdubm5i3LhxYs2aNWLFihWif//+wsbGRowaNUrqEos0dOhQ4e/vX6LnOjo6iqFDh+bbn5ubK7KysoRarS5dcQZau3atACA+/vhj8d1334mvv/5azJs3T3Tt2lXIZDLh7+8vzp8/r/ecnJwckZWVZdB5Tp48KQCItWvXGvS87OxskZ2drfs6JiZGABA//vijQe2UtDalUimePHlitHOVhZCQENGmTZtiHdujRw8hl8vFoEGDxMqVK0VUVJR46623RLVq1Qz+tzFU9+7dS/z/piJq3769aNCggdRlFGrs2LGioF+TsbGxAoD45JNPJKjKPFlLGYSoZGJjY9G/f3/4+/vjwIED8PHx0T02duxYXL9+HTt27JCwQmnI5XLI5XLJzt+tWze0aNFC9/XUqVNx4MAB9OjRAz179sSlS5dgb28PALC2toa1ddn+98vMzISDgwNsbW3L9DzPY2NjI+n5iyMpKQn169d/7nEnT57E9u3bMXfuXEybNk3vsRUrVuDx48dlVCGReWM3jQlauHAh0tPT8dVXX+kFEa1atWph/PjxAJ72ba5bty7fcTKZTO8SpHYcw9WrVzFo0CC4urrC09MTM2bMgBAC8fHxeOWVV+Di4gJvb28sWrRIr73CxmxoxygcPHiwyNf16aefonXr1qhcuTLs7e3RvHnzfOMaZDIZMjIy8M033+i6RYYNG1bg+Xv06IGaNWsWeK7Q0FC94AAA33//PZo3bw57e3tUqlQJ/fv3R3x8fJE1P0+nTp0wY8YM3Lp1C99//71uf0FjRvbt24e2bdvCzc0NTk5OqFu3ru4X3sGDB9GyZUsAwPDhw3WvXfvvqr3cffr0abz44otwcHDQPffZMSNaKpUK06ZNg7e3NxwdHdGzZ898r7dGjRq69zevvG0+r7aCxoxkZGTg3XffhZ+fHxQKBerWrYtPP/0U4pmbiMtkMrz99tvYtm0bGjZsCIVCgQYNGmD37t0Fv+HPSEpKwogRI+Dl5QU7Ozs0adIE33zzje5x7fdmbGwsduzYoau9sHFHN27cAAC0adMm32NyuRyVK1fW23f37l288cYb8PLy0tX+9ddf6x2jrWHTpk2YO3cuqlWrBjs7O3Tu3BnXr1/XHdehQwfs2LEDt27d0tWZ933Nzs7GrFmzUKtWLSgUCvj5+eH9999HdnZ2id/Tu3fvYsSIEfD19YVCoUBAQADGjBkDpVKpO+bx48eYMGGC7t+yVq1aWLBgAdRqdYHvYUns2rUL7dq1g6OjI5ydndG9e3f8/fffescMGzYMTk5OuHv3Lnr16gUnJyd4enpi8uTJUKlUesf+888/GDx4sK6bbejQoTh//ny+79uVK1fq3jPtx7PWrFmDwMBAKBQKtGzZEidPnjTa67YkvDJign799VfUrFkTrVu3LpP2+/Xrh3r16mH+/PnYsWMH5syZg0qVKuHzzz9Hp06dsGDBAvzwww+YPHkyWrZsiRdffNEo5126dCl69uyJgQMHQqlUYsOGDXjttdewfft2dO/eHQDw3XffYeTIkWjVqhVGjx4NAAgMDCz0dQwZMgQnT57U/bIEgFu3buH333/HJ598ots3d+5czJgxA3379sXIkSORnJyM5cuX48UXX8TZs2fh5uZW4tc1ePBgTJs2DXv37sWoUaMKPObvv/9Gjx490LhxY3z88cdQKBS4fv06jh49CgCoV68ePv74Y8ycOROjR49Gu3btAEDve+Cff/5Bt27d0L9/fwwaNAheXl5F1jV37lzIZDJ88MEHSEpKQlRUFMLCwnDu3DndFZziKE5teQkh0LNnT8TExGDEiBEIDg7Gnj178N577+Hu3btYsmSJ3vFHjhzB1q1b8d///hfOzs5YtmwZXn31Vdy+fTvfL/+8srKy0KFDB1y/fh1vv/02AgIC8OOPP2LYsGF4/Pgxxo8fj3r16uG7777DxIkTUa1aNbz77rsAAE9PzwLb9Pf3BwD88MMPaNOmTZFXtxITE/HCCy/ofvl7enpi165dGDFiBFJTU/MNkp0/fz6srKwwefJkpKSkYOHChRg4cCD++OMPAMD06dORkpKCO3fu6N4jJycnAIBarUbPnj1x5MgRjB49GvXq1cOff/6JJUuW4OrVq/nGWBXnPb137x5atWqFx48fY/To0QgKCsLdu3exefNmZGZmwtbWFpmZmWjfvj3u3r2LN998E9WrV8exY8cwdepU3L9/3yjjW7777jsMHToU4eHhWLBgATIzM/HZZ5+hbdu2OHv2rF4gU6lUCA8PR0hICD799FPs378fixYtQmBgIMaMGaN7ryIiInDixAmMGTMGQUFB+PnnnzF06FC987755pu4d+8e9u3bh++++67A2tavX4+0tDS8+eabkMlkWLhwIXr37o2bN2+axBXBCkXibiIyUEpKigAgXnnllWIdr+3bLKgvG4CYNWuW7utZs2YJAGL06NG6fbm5uaJatWpCJpOJ+fPn6/Y/evRI2Nvb643d0I6diI2N1TuPdoxCTEyMbl9BY0YyMzP1vlYqlaJhw4aiU6dOevsLGzPy7PlTUlKEQqEQ7777rt5xCxcuFDKZTNy6dUsIIURcXJyQy+Vi7ty5esf9+eefwtraOt/+ws578uTJQo9xdXUVTZs21X2tfa+1lixZIgCI5OTkQtsoalxG+/btBQCxevXqAh9r37697mvtv0fVqlVFamqqbv+mTZsEALF06VLdPn9//wLf62fbLKq2Z/+tt23bJgCIOXPm6B3Xp08fIZPJxPXr13X7AAhbW1u9fefPnxcAxPLly/OdK6+oqCgBQHz//fe6fUqlUoSGhgonJye91+7v7y+6d+9eZHtCCKFWq3XvtZeXlxgwYIBYuXKl7nsprxEjRggfHx/x4MEDvf39+/cXrq6uuu937b9HvXr19Mb2LF26VAAQf/75p25fYWNGvvvuO2FlZSV+++03vf2rV68WAMTRo0d1+4r7ng4ZMkRYWVkV+H2tHZc1e/Zs4ejoKK5evar3+JQpU4RcLhe3b9/O99y8njdmJC0tTbi5ueUbA5eQkCBcXV319g8dOlQ3diuvpk2biubNm+u+3rJliwAgoqKidPtUKpXo1KlTvu/h540ZqVy5snj48KFu/88//ywAiF9//bXI1035sZvGxKSmpgIAnJ2dy+wcI0eO1G3L5XK0aNECQgiMGDFCt9/NzQ1169bFzZs3jXbevH+NP3r0CCkpKWjXrh3OnDlTovZcXFzQrVs3bNq0Se/y/8aNG/HCCy+gevXqAICtW7dCrVajb9++ePDgge7D29sbtWvXRkxMTOleGDR/wRY1q0Z75eXnn38u8eVthUKB4cOHF/v4IUOG6H0f9enTBz4+Pti5c2eJzl9cO3fuhFwux7hx4/T2v/vuuxBCYNeuXXr7w8LC9K5+NW7cGC4uLs/93tu5cye8vb0xYMAA3T4bGxuMGzcO6enpOHTokMG1y2Qy7NmzB3PmzIG7uzv+97//YezYsfD390e/fv10Y0aEENiyZQsiIiIghND7vgoPD0dKSkq+7+vhw4frje/RXmEqzv+xH3/8EfXq1UNQUJDeuTp16gQA+b6Hn/eeqtVqbNu2DREREfm6M7Xvg/a87dq1g7u7u955w8LCoFKpcPjw4efWXpR9+/bh8ePHGDBggF77crkcISEhBf7ffOutt/S+bteund57uHv3btjY2OhdpbSyssLYsWMNrq9fv35wd3fXOxdQvH8z0sduGhPj4uICAEabLloQ7S9pLVdXV9jZ2cHDwyPf/n/++cdo592+fTvmzJmDc+fO6fVzl2Y9jn79+mHbtm04fvw4WrdujRs3buD06dN6l4+vXbsGIQRq165dYBvGuNyanp6OKlWqFFnnl19+iZEjR2LKlCno3LkzevfujT59+sDKqnh/M1StWtWgwarPvl6ZTIZatWqV+Tott27dgq+vb75AXa9ePd3jeT37/QgA7u7uePTo0XPPU7t27XzvX2HnKS6FQoHp06dj+vTpuH//Pg4dOoSlS5di06ZNsLGxwffff4/k5GQ8fvwYa9aswZo1awpsJykpSe/rZ1+n9pfc814noPkevnTpUqHdS887l/Z82nMlJycjNTX1udNur127hgsXLhT7vIa6du0aAOhC1bO0Pw+17Ozs8tXy7PfKrVu34OPjAwcHB73jatWqZXB9pfk3I30MIybGxcUFvr6++Ouvv4p1fGG/yJ8d0JVXQTNSCpulkveKQ0nOpfXbb7+hZ8+eePHFF7Fq1Sr4+PjAxsYGa9euxfr165/7/MJERETAwcEBmzZtQuvWrbFp0yZYWVnhtdde0x2jVqshk8mwa9euAl+ntl++pO7cuYOUlJQif9jZ29vj8OHDiImJwY4dO7B7925s3LgRnTp1wt69e4s1S8iQcR7FVdS/aXnNXCrO955UfHx80L9/f7z66qto0KABNm3ahHXr1umubg0aNCjfWAStxo0b631dmtepVqvRqFEjLF68uMDH/fz8jHauZ8/bpUsXvP/++wU+XqdOHYPaK6h9QDNuxNvbO9/jz47ZKe/ZdBX5e9PUMIyYoB49emDNmjU4fvw4QkNDizxWm9SfnXJY0r8Ky+pcW7ZsgZ2dHfbs2QOFQqHbv3bt2nzHGnKlxNHRET169MCPP/6IxYsXY+PGjWjXrh18fX11xwQGBkIIgYCAgFL/8CyIdvBbeHh4kcdZWVmhc+fO6Ny5MxYvXox58+Zh+vTpiImJQVhYmNFXbNX+1aklhMD169f1fkm6u7sXOF311q1bejOVDKnN398f+/fvR1pamt7VEe2CYdpBoqXl7++PCxcuQK1W610dMfZ5AM3Vs8aNG+PatWt48OABPD094ezsDJVKhbCwMKOdp7D3OTAwEOfPn0fnzp2N8n3i6ekJFxeX5/7RExgYiPT0dKO+xmfbB4AqVaoY7Rz+/v6IiYnRTX3XyjtzSau8VkkmTu01Se+//z4cHR0xcuRIJCYm5nv8xo0bWLp0KQDNlRQPD498fberVq0yel3aHxx5z6VSqQq9TJ2XXC6HTCbTu4oSFxdX4Eqrjo6OBq3n0K9fP9y7dw9ffvklzp8/j379+uk93rt3b8jlcnz00Uf5/qIRQpSqK+rAgQOYPXs2AgICMHDgwEKPe/jwYb59wcHBAKDrsnJ0dASQP+yV1LfffqvX3bd582bcv38f3bp10+0LDAzE77//rjeVc/v27fmmABtS28svvwyVSoUVK1bo7V+yZAlkMpne+Uvj5ZdfRkJCAjZu3Kjbl5ubi+XLl8PJyQnt27c3uM1r167h9u3b+fY/fvwYx48fh7u7Ozw9PSGXy/Hqq69iy5YtBf5CT05ONvjcgOZ9TklJybe/b9++uHv3Lr744ot8j2VlZSEjI8Og81hZWaFXr1749ddfcerUqXyPa/+f9O3bF8ePH8eePXvyHfP48WPk5uYadN5nhYeHw8XFBfPmzUNOTk6+x0vyPoaHhyMnJ0fvvVKr1bppvHkZ+/8cFY5XRkxQYGAg1q9fr5uCO2TIEDRs2BBKpRLHjh3TTV/UGjlyJObPn4+RI0eiRYsWOHz4MK5evWr0uho0aIAXXngBU6dOxcOHD1GpUiVs2LChWD+QunfvjsWLF+Oll17C66+/jqSkJKxcuRK1atXChQsX9I5t3rw59u/fj8WLF8PX1xcBAQEICQkptO2XX34Zzs7OmDx5su6XRF6BgYGYM2cOpk6diri4OPTq1QvOzs6IjY3FTz/9hNGjR2Py5MnPfQ27du3C5cuXkZubi8TERBw4cAD79u2Dv78/fvnlF9jZ2RX63I8//hiHDx9G9+7d4e/vj6SkJKxatQrVqlVD27ZtdXW6ublh9erVcHZ2hqOjI0JCQhAQEPDc2gpSqVIltG3bFsOHD0diYiKioqJQq1YtvYF9I0eOxObNm/HSSy+hb9++uHHjBr7//vt806kNqS0iIgIdO3bE9OnTERcXhyZNmmDv3r34+eefMWHChEKnahtq9OjR+PzzzzFs2DCcPn0aNWrUwObNm3H06FFERUWVaBD4+fPn8frrr6Nbt25o164dKlWqhLt37+Kbb77BvXv3EBUVpbt0P3/+fMTExCAkJASjRo1C/fr18fDhQ5w5cwb79+8vMIA+T/PmzbFx40ZMmjQJLVu2hJOTEyIiIjB48GBs2rQJb731FmJiYtCmTRuoVCpcvnwZmzZtwp49ewociFqUefPmYe/evWjfvr1uuvD9+/fx448/4siRI3Bzc8N7772HX375BT169MCwYcPQvHlzZGRk4M8//8TmzZsRFxeXb6zZs5KTkzFnzpx8+7UB/rPPPsPgwYPRrFkz9O/fH56enrh9+zZ27NiBNm3a5Au1z9OrVy+0atUK7777Lq5fv46goCD88ssvun+PvFdDmjdvDgAYN24cwsPDIZfL0b9/f4POR8VU/hN4yFiuXr0qRo0aJWrUqCFsbW2Fs7OzaNOmjVi+fLne8tuZmZlixIgRwtXVVTg7O4u+ffuKpKSkQqf2Pju9dOjQocLR0THf+Qualnfjxg0RFhYmFAqF8PLyEtOmTRP79u0r1tTer776StSuXVsoFAoRFBQk1q5dm28KrBBCXL58Wbz44ovC3t5eANBNPS1sarEQQgwcOFAAEGFhYYW+n1u2bBFt27YVjo6OwtHRUQQFBYmxY8eKK1euFPqcvOfVftja2gpvb2/RpUsXsXTpUr0ppFrPvq7o6GjxyiuvCF9fX2Frayt8fX3FgAED8k2Z/Pnnn0X9+vWFtbW13jTEoqZIFja193//+5+YOnWqqFKlirC3txfdu3cvcIrqokWLRNWqVYVCoRBt2rQRp06dytdmUbUV9G+dlpYmJk6cKHx9fYWNjY2oXbu2+OSTT/It5Q9AjB07Nl9NhU05flZiYqIYPny48PDwELa2tqJRo0YFTj8u7tTexMREMX/+fNG+fXvh4+MjrK2thbu7u+jUqZPYvHlzgcePHTtW+Pn5CRsbG+Ht7S06d+4s1qxZozumsOX5C5qWn56eLl5//XXh5uYmAOi9r0qlUixYsEA0aNBAKBQK4e7uLpo3by4++ugjkZKSojvOkPf01q1bYsiQIcLT01MoFApRs2ZNMXbsWL0pyGlpaWLq1KmiVq1awtbWVnh4eIjWrVuLTz/9VCiVyiLfT+006YI+OnfurPcehYeHC1dXV2FnZycCAwPFsGHDxKlTp3THFPZzqqCfIcnJyeL1118Xzs7OwtXVVQwbNkwcPXpUABAbNmzQHZebmyveeecd4enpKWQyma6dopaDf/bnKhWPTAiOtCEiIsu2bds2/Oc//8GRI0cKXGGXyhbDCBERWZSsrCy92WcqlQpdu3bFqVOnkJCQUCYz06hoHDNCREQW5Z133kFWVhZCQ0ORnZ2NrVu34tixY5g3bx6DiER4ZYSIiCzK+vXrsWjRIly/fh1PnjxBrVq1MGbMGLz99ttSl2axGEaIiIhIUlxnhIiIiCTFMEJERESSMokBrGq1Gvfu3YOzszOX5yUiIjIRQgikpaXB19e3yJt+mkQYuXfvXr4bPREREZFpiI+PR7Vq1Qp93CTCiHbZ5vj4+Hy3jCYiIqKKKTU1FX5+fs+9/YJJhBFt14yLiwvDCBERkYl53hALDmAlIiIiSTGMEBERkaQYRoiIiEhSJjFmpDhUKhVycnKkLqNCk8vlsLa25vRoIiKqUMwijKSnp+POnTvgyvbP5+DgAB8fH9ja2kpdChEREQAzCCMqlQp37tyBg4MDPD09+Vd/IYQQUCqVSE5ORmxsLGrXrl3kAjRERETlxeTDSE5ODoQQ8PT05K2fn8Pe3h42Nja4desWlEol7OzspC6JiIjIfAaw8opI8fBqCBERVTT8zURERESSYhghIiIiSRkcRg4fPoyIiAj4+vpCJpNh27Ztz33OwYMH0axZMygUCtSqVQvr1q0rQalERERkjgwOIxkZGWjSpAlWrlxZrONjY2PRvXt3dOzYEefOncOECRMwcuRI7Nmzx+BiiYiIyPwYPJumW7du6NatW7GPX716NQICArBo0SIAQL169XDkyBEsWbIE4eHhBT4nOzsb2dnZuq9TU1MNLZPI8ty+DWzaBKSna77WrruTd/2dovaV5DnGartmTWD8+IJfF1UYQgiohRpqoYZKqKBSq8p8W0BACKH7DEBvn7auovY9+5zybMcYzynt48V9zsTQiajhVqP03yglUOZTe48fP46wsDC9feHh4ZgwYUKhz4mMjMRHH31UshMKAWRmluy5peXgABgwq0etVmPBggVYs2YNEhISUKdOHcyYMQN9+vTBwYMH0bFjR+zfvx8ffPABLl68iODgYKxduxZ169bVtTFnzhwsW7YMWVlZ6NevHzw8PLB7926cO3euDF4gVThCAEePAkuXAlu3Amq11BWVTNu2DCPPkaPKQZoyDWnZaUhXpiNN+e/n7DS9be1jadlpSM/Rfzw7N1v3i14t1FCpVQZtq4WJfn9RsQxoNMB8w0hCQgK8vLz09nl5eSE1NRVZWVkFrg0ydepUTJo0Sfd1amoq/Pz8infCzEzAyalUNZdYejrg6FjswyMjI/H9999j9erVqF27Ng4fPoxBgwbB09NTd8z06dOxaNEieHp64q233sIbb7yBo0ePAgB++OEHzJ07F6tWrUKbNm2wYcMGLFq0CAEBAUZ/aVTBZGcDGzdqQsiZM0/3d+wI1Kv39Ou84Vi7XdC+kh5b2se12/7+MCdCCDzJfVJ4SChgu9B9/4YJpUop9csqFhlkkFvJIZfJYSWzKvW2DDLIZDK9zwD09mnPW9S+Z59Tnu089zklaMPQx4vzHF9nX+N8E5RAhVz0TKFQQKFQSF1GmcrOzsa8efOwf/9+hIaGAgBq1qyJI0eO4PPPP8fo0aMBAHPnzkX79u0BAFOmTEH37t3x5MkT2NnZYfny5RgxYgSGDx8OAJg5cyb27t2LdO1lejI/CQnA6tWaj8REzT47O2DQIGDcOKBRI2nrM2NKlRIXky/iXMI5nE84j+TM5CKvTqiEqkzqUMgVcFY4w9nWGU62TnBW/PvZ1lmzbVPAPlsnONk6wc7aTvNLXiaH3Epu9G0rmRXXfKISKfMw4u3tjUTtD81/JSYmwsXFpWxWTHVweNpnXt4cHIp96PXr15GZmYkuXbro7VcqlWjatKnu68aNG+u2fXx8AABJSUmoXr06rly5gv/+9796z2/VqhUOHDhQkuqpIjt9WnMVZONGQPnvX8hVqwJjxwKjRgEeHtLWZ2ZSs1NxLuGc7uNswln8nfQ3ctSG34zT0cax8PBQWKD4d1v7mHbbydYJNnKbMnjFRNIq8zASGhqKnTt36u3bt2+f7mqA0clkBnWVSEV79WLHjh2oWrWq3mMKhQI3btwAANjYPP3Bo/2LQ22q4wLIMLm5wLZtmhBy5MjT/aGhmvEVvXsDNvzFVBpCCNxLu6cLHNrPNx/dLPB4Nzs3NPVuiiZeTeDn6ldgYMi77WjrCCsZl3Mieh6Dw0h6ejquX7+u+zo2Nhbnzp1DpUqVUL16dUydOhV3797Ft99+CwB46623sGLFCrz//vt44403cODAAWzatAk7duww3qswQfXr14dCocDt27d13TB5acNIUerWrYuTJ09iyJAhun0nT540ap0kgYcPgS+/BFau1MyQAQBra6BvX00IadVK2vpMlEqtwrWH1zSB4/5ZXfhIzkwu8PjqrtUR7B2Mpt5NdZ+ru1ZnNwRRGTA4jJw6dQodO3bUfa0daDp06FCsW7cO9+/fx23tD1AAAQEB2LFjByZOnIilS5eiWrVq+PLLLwud1mspnJ2dMXnyZEycOBFqtRpt27ZFSkoKjh49ChcXF/gXY0DfO++8g1GjRqFFixZo3bo1Nm7ciAsXLqBmzZrl8ArI6C5eBJYtA779FsjK0uzz8ADeegsYMwbwlW5wmanJysnCX0l/6V3tuJB4AZk5+WfayWVyBHkEoalPUwR7BaOpj+bKR2WHyhJUTmSZDA4jHTp00M1JLkhBq6t26NABZ8+eNfRUZm/27Nnw9PREZGQkbt68CTc3NzRr1gzTpk0rVlfMwIEDcfPmTUyePBlPnjxB3759MWzYMJw4caIcqiejUKuB3bs1XTF79z7d36SJ5irIgAGaAapUqH8y/9Eb23Eu4RwuP7hc4ABSBxsHNPFqonfFo2GVhrC34R2/iaQkE0UliwoiNTUVrq6uSElJgYuLi95jT548QWxsLAICAmDHH9ro0qULvL298d133xX4ON+vCiItDfjmG2D5cuDqVc0+KyvglVc0IeTFFw1as8YSCCFwK+UWzt4/qxc84lPjCzze08ETTX2a6nWz1KpUC3IreTlXTmS5ivr9nVeFnNpLxZOZmYnVq1cjPDwccrkc//vf/7B//37s27dP6tKoMLGxmgDy1VeAdmVhV1dgxAjg7bcBrhEDQLPA1+UHl/W6Wc4lnMPjJ48LPD7QPVCvmyXYOxg+Tj4c30FkIhhGTJhMJsPOnTsxd+5cPHnyBHXr1sWWLVvyrXhLEhMCOHRI0xXzyy9PV0mtU0ezNsjQodIt1FcBpCvTcT7hvF7o+CvpL2SrsvMda2Nlg4ZVGup1szTxbgIXReF/cRFRxccwYsLs7e2xf/9+qcugwjx5AqxfrwkhFy483R8erumKCQ/XdM1YqMycTET+FolPjn1SYPBwUbjkm81Sz7MebOW2ElRLRGWJYYTI2O7dA1atAj7/HHjwQLPPwQEYMkRzJSTvcu0WSAiBzRc349297+rGe/g6+6Kpt2Z8h7abJcAtgN0sRBaCYYTIWE6cAKKigB9/1CxYBgDVq2vGgowcCbi7S1peRfB30t94Z9c7iImLAQD4u/pjSfgS9ArqxeBBZMEYRohKIycH2LJF0xXz++9P97drp+mKeeUVzYJlFi7lSQo+PPghlp9YDpVQwc7aDh+0+QDvt3kfDjbFv40CEZkn/pQkKokHD4A1azTdMXfvavbZ2gL9+2tCSLNm0tZXQaiFGt+c+wZToqcgKSMJAPCfoP9gcfhiyW5VTkQVD8MIkSH+/FNzFeSHHzQDVAHAy0uzQupbb2m2CQBw8u5JvLPrHfxx9w8AQN3KdbGs2zJ0DewqcWVEVNEwjBA9j0oF7NihCSF574jcvLnmKkjfvoBCIV19FUxyRjKmRU/DV2e/goCAk60TZrWfhXEh4zgThogKxDAiESEE3nzzTWzevBmPHj3C2bNnERwcLHVZlFdqKvD115pFym7+exdXuVxzt9zx44HWrblKah656lysPrUaM2Jm6BYnG9R4EBaELYCvM++rQ0SFYxiRyO7du7Fu3TocPHgQNWvWhIeHh9QlUV6nTgGdOz9dJdXdHRg1Chg7VjNDhvQcijuEd3a9gz+T/gQABHsHY0W3FWhTvY3ElRGRKWAYkciNGzfg4+OD1q1bF/i4UqmErS0vaUtm0SJNEKldG3j3XWDQIMDRUeqqKpw7qXfw3r73sOGvDQCASvaVMLfTXIxqNor3gCGiYjO7MCKEKPA24eXBwcahWGslDBs2DN988w0AzZLu/v7+qFGjBho2bAhra2t8//33aNSoEWJiYrB48WKsXbsWN2/eRKVKlRAREYGFCxfCyYKXDy9zKtXTO+iuXQu04V/3z8rOzcaS35dgzuE5yMjJgAwyvNn8TczpNAeVHSpLXR4RmRizCyOZOZlwipTmF3X61HQ42j7/r+elS5ciMDAQa9aswcmTJyGXy/Haa6/hm2++wZgxY3D06FHdsVZWVli2bBkCAgJw8+ZN/Pe//8X777+PVatWleVLsWwnTwIPHwJubkBIiNTVVDg7r+3EhN0TcO3hNQBAa7/WWN5tOZr5cDozEZWM2YURU+Dq6gpnZ2fI5XJ4e3vr9teuXRsLFy7UO3bChAm67Ro1amDOnDl46623GEbK0q5dms9dunDBsjxuPLyBCXsmYPvV7QAAbydvLAxbiEGNB3H1VCIqFbP7Setg44D0qemSnbs0mjdvnm/f/v37ERkZicuXLyM1NRW5ubl48uQJMjMz4eDAlSvLxO7dms/duklbRwWRocxA5BHNDe2UKiWsrawxIWQCZrSfwbvlEpFRmF0YkclkxeoqqYgcnxkgGRcXhx49emDMmDGYO3cuKlWqhCNHjmDEiBFQKpUMI2UhOVnTTQMAL70kbS0SK+iGdl1qdsHSl5ainqdl3+yPiIzL7MKIOTl9+jTUajUWLVoEq39vNb9p0yaJqzJz+/YBQgBNmgA+PlJXIxne0I6IyhPDSAVWq1Yt5OTkYPny5YiIiMDRo0exevVqqcsyb9rxIhbaRfP4yWN8ePBDrDixQu+Gdh+0+QD2NvZSl0dEZspK6gKocE2aNMHixYuxYMECNGzYED/88AMiIyOlLst8qdXAnj2abQvrolELNdaeXYu6K+pi6R9LoRIq/CfoP7g09hI+7PAhgwgRlSmZEEJIXcTzpKamwtXVFSkpKXBx0R8w9+TJE8TGxiIgIAB2dnYSVWg6+H4V4dQpoGVLwNkZ+OcfwMZG6orKBW9oR0Rlpajf33mxm4ZIS9tFExZmEUEkOSMZU6On4uuzX/OGdkQkKYYRIi0LmdKbq87FZyc/w8yDM3U3tBvceDAWhC2Aj7PlDtolIukwjBABmhVXf/9ds23G40V4QzsiqogYRogAYP9+zQDWBg0APz+pqzE63tCOiCoyswkjJjAOt0Lg+1QIM53Sm52bjcXHF2POb3OQmZPJG9oRUYVk8mFELtf8VadUKmFvz+mHz5OZqbmjsY0FDNAsNiGejhcxoy6andd2Yvzu8bj+8DoAzQ3tVnRbgaY+TSWujIhIn8mHEWtrazg4OCA5ORk2Nja6lUpJnxACmZmZSEpKgpubmy7EEYDz54GEBMDREWjbVupqSo03tCMiU2PyYUQmk8HHxwexsbG4deuW1OVUeG5ubnp3CiY8vSrSqROgUEhbSynkqHLw8aGPsfDYQt7QjohMismHEQCwtbVF7dq1oVQqpS6lQrOxseEVkYKYyXiRCbsnYNWpVQA0N7Rb1m0ZgjyCJK6KiOj5zCKMAICVlRVXFCXDpaQAR49qtk14vMjmi5t1QeTbXt+yS4aITIrZhBGiEomOBlQqoG5dICBA6mpK5OajmxjxywgAwAdtPsDgJoMlroiIyDAc7UmWTdtFY6JXRZQqJfpv7o/U7FSEVgvF7I6zpS6JiMhgDCNkufJO6TXR8SJT9k/ByXsn4W7njg19NsBGzinbRGR6GEbIcv39N3DnDmBvD7RvL3U1Bvvlyi9Y8vsSAMC6XutQ3bW6xBUREZUMwwhZLu1VkQ4dABMb/Hw75TaGbRsGAJgQMgE96/aUtiAiolJgGCHLZaJTenNUORiwZQAePXmEFr4tsKDLAqlLIiIqFYYRskxpacBvv2m2TWzw6syYmTgWfwwuChds7LMRtnJbqUsiIioVhhGyTDExQE4OEBgI1K4tdTXFtuf6Hsw/Oh8A8GXEl6jpXlPiioiISo9hhCyTCU7pvZd2D4N/0qwhMqbFGLzW4DWJKyIiMg6GEbI8JjilV6VWYeDWgUjOTEZjr8ZYHL5Y6pKIiIyGYYQsz5UrQFwcYGurmUljAmYfno2DcQfhaOOITX02wc7atGb/EBEVhWGELI/2qkj79oCjo7S1FMOB2AP4+NDHAIDPe3yOuh51Ja6IiMi4GEbI8pjQlN6kjCQM3DoQAgJvBL+BgY0HSl0SEZHRMYyQZcnMBA4d0mxX8MGraqHG4J8GIyE9AfU962NZt2VSl0REVCYYRsiyHDwIZGcD/v5AUJDU1RRpwZEF2HtjL+yt7bGpzyY42lb8LiUiopJgGCHLkndKr0wmbS1FOHL7CGbEzAAArHh5BRpUaSBxRUREZYdhhCyLCUzp/SfzHwzYMgAqocLARgMxPHi41CUREZUphhGyHNevaz5sbIBOnaSupkBCCAz7eRjupN5Bncp18Fn3zyCrwFdwiIiMgWGELIf2qkjbtoCzs7S1FGLJ70uw/ep2KOQKbOyzEc6KilknEZExMYyQ5ajgU3pP3D2BD/Z/AABYEr4Ewd7B0hZERFROGEbIMjx5ork5HlAhp/Q+fvIY/Tb3Q646F33q98FbLd6SuiQionLDMEKW4fBhICsLqFoVaNhQ6mr0CCEw8peRiHschwC3AHwZ8SXHiRCRRWEYIcugHS9SAaf0rjq5ClsubYGNlQ029tkIVztXqUsiIipXDCNkGSroeJGz989i0t5JAICFXRaiZdWWEldERFT+ShRGVq5ciRo1asDOzg4hISE4ceJEkcdHRUWhbt26sLe3h5+fHyZOnIgnT56UqGAig8XFAZcvA3I50Lmz1NXopGWnoe/mvlCqlOhZtyfGh4yXuiQiIkkYHEY2btyISZMmYdasWThz5gyaNGmC8PBwJCUlFXj8+vXrMWXKFMyaNQuXLl3CV199hY0bN2LatGmlLp6oWLRdNK1bA25ukpaiJYTAm9vfxPWH1+Hn4oe1r6zlOBEislgGh5HFixdj1KhRGD58OOrXr4/Vq1fDwcEBX3/9dYHHHzt2DG3atMHrr7+OGjVqoGvXrhgwYMBzr6YQGU3eJeAriK/OfoX//fU/yGVybOizAZXsK0ldEhGRZAwKI0qlEqdPn0ZYWNjTBqysEBYWhuPHjxf4nNatW+P06dO68HHz5k3s3LkTL7/8cqHnyc7ORmpqqt4HUYkolUB0tGa7gowX+SvpL7yz6x0AwNxOc9Har7XEFRERScvakIMfPHgAlUoFLy8vvf1eXl64fPlygc95/fXX8eDBA7Rt2xZCCOTm5uKtt94qspsmMjISH330kSGlERXsyBEgIwPw9gaCg6WuBhnKDPT9sS+e5D7BS7Vewntt3pO6JCIiyZX5bJqDBw9i3rx5WLVqFc6cOYOtW7dix44dmD17dqHPmTp1KlJSUnQf8fHxZV0mmSvteJHw8AoxpfedXe/g0oNL8HX2xbe9voWVjBPaiIgMujLi4eEBuVyOxMREvf2JiYnw9vYu8DkzZszA4MGDMXLkSABAo0aNkJGRgdGjR2P69Omwssr/w1ihUEChUBhSGlHBKtCU3u/Of4e159bCSmaF9b3Xw9PRU+qSiIgqBIP+LLO1tUXz5s0Rre2DB6BWqxEdHY3Q0NACn5OZmZkvcMjlcgCaGQVEZSY+HvjrL8DKCujSRdJSLj+4jDE7xgAAZrWfhfY12ktaDxFRRWLQlREAmDRpEoYOHYoWLVqgVatWiIqKQkZGBoYPHw4AGDJkCKpWrYrIyEgAQEREBBYvXoymTZsiJCQE169fx4wZMxAREaELJURlYs8ezeeQEKCSdLNVsnKy0G9zP2TkZKBTQCdMbzddslqIiCoig8NIv379kJycjJkzZyIhIQHBwcHYvXu3blDr7du39a6E/N///R9kMhn+7//+D3fv3oWnpyciIiIwd+5c470KooJUkCm9k/ZMwoXEC6jiWAXf/+d7yK0YwomI8pIJE+grSU1NhaurK1JSUuDi4iJ1OWQKcnIADw8gNRU4cQJoKc0y65v+3oR+m/tBBhn2DNqDLoHSdhcREZWn4v7+5lB+Mk/Hj2uCiIcH0Ly5JCXceHgDI3/RDNye2nYqgwgRUSEYRsg85Z3SW8CMrbKWnZuNfpv7IU2ZhrbV2+Kjjlw3h4ioMAwjZJ4kntL7/r73cfr+aVS2r4z/vfo/WFsZPDyLiMhiMIyQ+bl/Hzh3TrPIWdeu5X76bZe3YdmJZQCAb3p9g2ou1cq9BiIiU8IwQuZHO6W3RQvAs3wXFrv1+BaG/6yZ5v5u6LvoXqd7uZ6fiMgUMYyQ+ZFoSm+OKgf9t/TH4yeP0apqK8zrPK9cz09EZKoYRsi85OYC+/Zptst5vMj0A9Px+53f4apwxcY+G2Erty3X8xMRmSqGETIvJ04Ajx4B7u5Aq1bldtqd13bik2OfAAC+fuVr1HCrUW7nJiIydQwjZF60U3q7dgXK6XYDd1LvYMhPQwAAb7d8G73r9S6X8xIRmQuGETIv5TxeJFedi9e3vI5/sv5BM59m+LTrp+VyXiIic8IwQuYjKQk4dUqzXU5h5KODH+G327/B2dYZG/tshMJaUS7nJSIyJwwjZD727tV8btoU8PYu89Ptv7kfc3/T3PBxTcQa1KpUq8zPSURkjhhGyHxox4uUw1WRhPQEDNo6CAICo5qNQv+G/cv8nERE5ophhMyDWv10sbMyntKrUqswaOsgJGYkomGVhlj60tIyPR8RkbljGCHzcOoU8OAB4OICvPBCmZ4q8kgkomOj4WDjgE19NsHexr5Mz0dEZO4YRsg8aLtounQBbGzK7DSHbx3GrIOzAACrXl6Fep71yuxcRESWgmGEzEM5TOlNzkjGgC0DoBZqDGkyBEODh5bZuYiILAnDCJm+f/7RrLwKlFkYUQs1hm4bintp9xDkEYSVL68sk/MQEVkihhEyffv2aQawNmwIVKtWJqdYdGwRdl3fBTtrO2zssxFOtk5lch4iIkvEMEKmTztepIxm0RyPP45pB6YBAJa+tBSNvRqXyXmIiCwVwwiZNrW6TMPIo6xH6L+lP3LVuejfsD9GNRtl9HMQEVk6hhEybefOAYmJgJMT0KaNUZsWQuCNX97A7ZTbCHQPxOc9PodMJjPqOYiIiGGETJ32qkjnzoCtrVGbXn5iObZd3gZbuS02vbYJLgoXo7ZPREQaDCNk2spoSu+5hHOYvHcyAODTLp+imU8zo7ZPRERPMYyQ6Xr8GDh+XLNt5DAS9XsUctQ56Fm3J95u9bZR2yYiIn0MI2S69u8HVCogKAioUcNozSpVSvx85WcAwOTQyRwnQkRUxhhGyHSV0SyamNgYPH7yGF6OXmjt19qobRMRUX4MI2SahCizMLL54mYAQO96vSG3khu1bSIiyo9hhEzTX38Bd+8CDg5Au3ZGazZXnYufLv8EAOhTv4/R2iUiosIxjJBp0s6i6dgRsLMzWrOHbx3GP1n/oLJ9Zbzo/6LR2iUiosIxjJBpKqMpvdoumv8E/QfWVtZGbZuIiArGMEKmJy0NOHJEs23E8SIqtQpbL20FwC4aIqLyxDBCpic6GsjNBWrVAgIDjdbssfhjSMxIhJudGzoGdDRau0REVDSGETI9ZTyL5pW6r8BWbtyl5YmIqHAMI2RahCiT8SJqocaWS1sAsIuGiKi8MYyQabl8Gbh9G1AogA4djNbsH3f+wN20u3C2dUaXml2M1i4RET0fwwiZFu1VkQ4dNGuMGIn2qkhE3QgorBVGa5eIiJ6PYYRMi3a8iBG7aIQQuvEifeqxi4aIqLwxjJDpyMgADh3SbBtx8Orp+6dxK+UWHGwcEF4r3GjtEhFR8TCMkOmIiQGUSs0deuvUMVqzWy5qumi61+4OBxvjdf0QEVHxMIyQ6cg7pVcmM0qTQghsvvRvFw1n0RARSYJhhExDGU3pvZB4AdcfXoedtR1erv2y0dolIqLiYxgh03D9OnDzJmBjA3TqZLRmtbNoXqr1EpxsnYzWLhERFR/DCJkG7VWRdu0AJ+OFBs6iISKSHsMImYYyWAL+YvJFXHpwCTZWNuhRp4fR2iUiIsMwjFDFl5WlmUkDGDWMaGfRdA3sClc7V6O1S0REhmEYoYrv0CHgyROgWjWgfn2jNctZNEREFQPDCFV8ZTCl99o/13Ah8QKsrazRs25Po7RJREQlwzBCFV8ZTOnVzqLpFNAJlewrGa1dIiIyHMMIVWw3bwJXrwLW1kDnzkZrlrNoiIgqDoYRqti0XTStWwOuxhlkGvsoFqfvn4aVzAq9gnoZpU0iIio5hhGq2MpgSu/WS1sBAO3928PT0dNo7RIRUckwjFDFlZ0NHDig2TbieBHOoiEiqlgYRqjiOnIEyMgAfHyAJk2M0uSd1Dv4/c7vkEGG/wT9xyhtEhFR6TCMUMWVdxaNkab0arto2lRvAx9nH6O0SUREpcMwQhVXGUzp5SwaIqKKp0RhZOXKlahRowbs7OwQEhKCEydOFHn848ePMXbsWPj4+EChUKBOnTrYuXNniQomC3H7NnDxImBlBXTpYpQmE9ITcOT2EQBA73q9jdImERGVnrWhT9i4cSMmTZqE1atXIyQkBFFRUQgPD8eVK1dQpUqVfMcrlUp06dIFVapUwebNm1G1alXcunULbm5uxqifzJV2Fs0LLwDu7kZp8qdLP0FAIKRqCPxc/YzSJhERlZ7BYWTx4sUYNWoUhg8fDgBYvXo1duzYga+//hpTpkzJd/zXX3+Nhw8f4tixY7CxsQEA1KhRo3RVk/krgym9nEVDRFQxGdRNo1Qqcfr0aYSFhT1twMoKYWFhOH78eIHP+eWXXxAaGoqxY8fCy8sLDRs2xLx586BSqQo9T3Z2NlJTU/U+yIIolcD+/ZptI40XSc5IxsG4gwCAV+u9apQ2iYjIOAwKIw8ePIBKpYKXl5fefi8vLyQkJBT4nJs3b2Lz5s1QqVTYuXMnZsyYgUWLFmHOnDmFnicyMhKurq66Dz8/XlK3KMePA2lpgKcn0KyZUZr8+crPUAs1mvk0Q4B7gFHaJCIi4yjz2TRqtRpVqlTBmjVr0Lx5c/Tr1w/Tp0/H6tWrC33O1KlTkZKSovuIj48v6zKpItHOogkP1wxgNQLOoiEiqrgMGjPi4eEBuVyOxMREvf2JiYnw9vYu8Dk+Pj6wsbGBXC7X7atXrx4SEhKgVCpha2ub7zkKhQIKhcKQ0sicGHm8yKOsR4iOjQYAvFqfXTRERBWNQX922traonnz5oiOjtbtU6vViI6ORmhoaIHPadOmDa5fvw61Wq3bd/XqVfj4+BQYRMjC3bsHnD+vWeSsa1ejNPnLlV+Qq85FoyqNUKdyHaO0SURExmPwNfBJkybhiy++wDfffINLly5hzJgxyMjI0M2uGTJkCKZOnao7fsyYMXj48CHGjx+Pq1evYseOHZg3bx7Gjh1rvFdB5kN7VaRlS8DDwyhNchYNEVHFZvDU3n79+iE5ORkzZ85EQkICgoODsXv3bt2g1tu3b8MqTz+/n58f9uzZg4kTJ6Jx48aoWrUqxo8fjw8++MB4r4LMh5G7aFKzU7H3xl4AnEVDRFRRyYQQQuoinic1NRWurq5ISUmBi4uL1OVQWcnN1cygefxYM6PmhRdK3eT6P9dj4NaBCPIIwsX/XoTMSPe4ISKi5yvu72/em4Yqjj/+0ASRSpU03TRGkHcWDYMIEVHFxDBCFYd2Sm/XrkCe2Vclla5Mx67rmjY5i4aIqOJiGKGKw8jjRXZd24UnuU8Q6B6IJl5NjNImEREZH8MIVQyJicDp05rt8HCjNJl3Fg27aIiIKi6GEaoY9uzRfG7WDHjmdgMlkZWThR1XdwDgLBoiooqOYYQqBiN30ey5sQcZORmo7lodLXxbGKVNIiIqGwwjJD2V6umVESPdpZezaIiITAfDCEnv1Cng4UPA1dUoa4tk52bj16u/AuAsGiIiU8AwQtLTTunt0gWwNnhR4Hz239yP1OxU+Dr74oVqpQ83RERUthhGSHpGHi+inUXzar1XYSXjtzgRUUXHn9QkrQcPgBMnNNtGmNKbo8rBz5d/BsBZNEREpoJhhKS1bx8gBNC4MVC1aqmbi4mLwaMnj1DFsQraVm9rhAKJiKisMYyQtLTjRYzVRfPvLJreQb0htyr9kvJERFT2GEZIOmr10/EiRpjSm6vOxU+XfwLAWTRERKaEYYSkc/YskJwMODsDrVuXurnfbv2GB5kPUNm+Mtr7tzdCgUREVB4YRkg62i6azp0BW9tSN6ftoukV1As2cptSt0dEROWDYYSkY8QpvWqhxtbLWwFwFg0RkalhGCFpPHoEHD+u2TbCeJFj8ceQkJ4AV4UrOtfsXOr2iIio/DCMkDT279cMYK1fH6hevdTNabtoXgl6Bbby0nf5EBFR+WEYIWlox4sY4aqIWqix5dIWAOyiISIyRQwjVP6EMOp4kZN3T+JO6h042Tqha2DXUrdHRETli2GEyt+FC8D9+4CDA9CuXamb03bR9KjTA3bWdqVuj4iIyhfDCJU/bRdNp06AQlGqpoQQuhvj9anXp7SVERGRBBhGqPwZsYvmbMJZxD2Og4ONA7rVNs6S8kREVL4YRqh8paYCR49qto0weFXbRdOtVjc42DiUuj0iIip/DCNUvqKjgdxcoE4doGbNUjUlhNCFkT712UVDRGSqGEaofBlxSu9fSX/h2sNrUMgV6F67e6nbIyIiaTCMUPkx8pRe7VWR8FrhcFY4l7o9IiKSBsMIlZ+LF4H4eMDODmhf+rvqchYNEZF5YBih8qPtounQAbC3L1VTl5Iv4WLyRdhY2SCibkTpayMiIskwjFD5MWIXjXb597CaYXCzcyt1e0REJB2GESof6enAb79pto04pZezaIiITB/DCJWPmBhAqdRM561du1RNXX94HecTz0Muk+OVuq8YqUAiIpIKwwiVj7xTemWyUjW15aKmi6ZjQEdUdqhc2sqIiEhiDCNU9oR4GkaMMaWXs2iIiMwKwwiVvatXgbg4wNYW6NixVE3FPY7DqXunYCWzQq+gXkYpj4iIpMUwQmVPO4vmxRcBR8dSNbX10lYAQLvq7eDl5FXayoiIqAJgGKGyt2OH5rMRV13lLBoiIvPBMEJlKy0NOHhQs92jR6maupN6B8fvHAcA9K7Xu5SFERFRRcEwQmVr714gJ0cznbdOnVI19dOlnwAArf1aw9fZ1xjVERFRBcAwQmVr+3bN54jSL9nOWTREROaJYYTKjlr9dLxIKbtoEtIT8NstzQqur9Z/tbSVERFRBcIwQmXnxAkgORlwdQXati1VU9sub4OAQEvflqjuWt1IBRIRUUXAMEJlR9tF89JLgI1NqZriLBoiIvPFMEJlRxtGStlF8yDzAQ7GHQQAvFqPXTREROaGYYTKxu3bwPnzgJVVqdcX+fnyz1AJFYK9gxFYKdBIBRIRUUXBMEJlQztwtXVroHLpbmbHWTREROaNYYTKxq+/aj6XsovmUdYj7L+5HwDHixARmSuGETK+jAzgwAHNdinDyK9Xf0WuOhcNPBugrkddIxRHREQVDcMIGV90NJCdDQQEAPXrl6opzqIhIjJ/DCNkfHm7aGSyEjeTmp2KPTf2AGAYISIyZwwjZFxGXHV1x9UdUKqUqFO5Dhp4NjBCcUREVBExjJBxnT0L3L8PODkB7duXqqm8s2hkpbjCQkREFRvDCBmXtouma1dAoShxMxnKDOy6tgsAu2iIiMwdwwgZl5FWXd11fReycrMQ4BaAYO/g0tdFREQVFsMIGc+9e8Dp05pBqy+/XKqm8s6iYRcNEZF5K1EYWblyJWrUqAE7OzuEhITgxIkTxXrehg0bIJPJ0KtXr5Kclio67cDVVq0AL68SN5OVk4XtVzVXWNhFQ0Rk/gwOIxs3bsSkSZMwa9YsnDlzBk2aNEF4eDiSkpKKfF5cXBwmT56Mdu3albhYquCM1EWz98ZeZORkwM/FDy19WxqhMCIiqsgMDiOLFy/GqFGjMHz4cNSvXx+rV6+Gg4MDvv7660Kfo1KpMHDgQHz00UeoWbNmqQqmCiorC9ivWbYdERGlako7i+bVeq+yi4aIyAIYFEaUSiVOnz6NsLCwpw1YWSEsLAzHjx8v9Hkff/wxqlSpghEjRhTrPNnZ2UhNTdX7oAouJgbIzASqVQMaNy5xM9m52fjlyi8A2EVDRGQpDAojDx48gEqlgtcz4wG8vLyQkJBQ4HOOHDmCr776Cl988UWxzxMZGQlXV1fdh5+fnyFlkhTydtGU4mpGdGw0UrNT4ePkg1C/UCMVR0REFVmZzqZJS0vD4MGD8cUXX8DDw6PYz5s6dSpSUlJ0H/Hx8WVYJZWaEE/DSGm7aP6dRdO7Xm9YyTjZi4jIElgbcrCHhwfkcjkSExP19icmJsLb2zvf8Tdu3EBcXBwi8vyCUqvVmhNbW+PKlSsIDAzM9zyFQgFFKRbMonJ24QIQHw/Y2wMdO5a4mRxVDrZd3gaAXTRERJbEoD89bW1t0bx5c0RHR+v2qdVqREdHIzQ0/yX1oKAg/Pnnnzh37pzuo2fPnujYsSPOnTvH7hdzob0qEhamCSQldDDuIB49eQRPB0+0q85ZV0RElsKgKyMAMGnSJAwdOhQtWrRAq1atEBUVhYyMDAwfPhwAMGTIEFStWhWRkZGws7NDw4YN9Z7v5uYGAPn2kwkzchfNf4L+A7mVvLRVERGRiTA4jPTr1w/JycmYOXMmEhISEBwcjN27d+sGtd6+fRtWVuzrtxhJScAff2i2S7Hqqkqtwk+XfwLALhoiIksjE0IIqYt4ntTUVLi6uiIlJQUuLi5Sl0N5rVsHDB8ONGumWQq+hA7GHUTHbzrC3c4diZMTYSO3MV6NREQkieL+/uYlDCodI3fR9ArqxSBCRGRhGEao5LKzgT17NNulWAJeLdTYemkrAHbREBFZIoYRKrnDh4H0dMDbW9NNU0LH44/jfvp9uChc0DmgsxELJCIiU8AwQiWXd9XVUgxa1nbR9KzbEwprri9DRGRpGEaoZIQAfv1Vs12KLhohBLZc2gIA6FOPXTRERJaIYYRK5tIlIDYWUCg0i52V0Ml7JxGfGg9HG0d0DexqxAKJiMhUMIxQyWi7aDp1AhwdS9yMtoumR50esLcp+eqtRERkuhhGqGSM1EWjDSOcRUNEZLkYRshw//wDHDum2e7evcTNnEs4h9jHsbC3tke3Wt2MVBwREZkahhEy3O7dgFoNNG4M+PuXuBntVZFutbvB0bbkXT1ERGTaGEbIcMbqorn0bxcNZ9EQEVk0hhEyTE6O5soIUKow8nfy37j6z1XYym3RvU7Ju3qIiMj0MYyQYY4eBVJSAE9PoFWrEjej7aIJDwyHi4I3PyQismQMI2QYbRfNyy8DcnmJm+EsGiIi0mIYIcPkXQK+hC4/uIy/k/+GtZU1IuqU7m6/RERk+hhGqPiuXtV82NgAXUu+WuqWi5rl38NqhsHd3t1Y1RERkYliGKHi014Vad8ecCn5OA/OoiEiorwYRqj4jNBFc+PhDZxLOAe5TI5Xgl4xUmFERGTKGEaoeB4/Bn77TbNdijCivUNvhxod4OHgYYTCiIjI1DGMUPHs2QPk5gL16gGBgSVuhrNoiIjoWQwjVDxG6KK59fgWTt47CRlk6BXUyzh1ERGRyWMYoedTqYCdOzXbESWfirv10lYAQDv/dvB28jZGZUREZAYYRuj5jh8HHj4E3N2B0NASN6OdRfNqvVeNVRkREZkBhhF6Pm0XTbdugLV1iZq4m3oXx+KPAQB61+ttrMqIiMgMMIzQ82nDSCm6aH66/BMAILRaKKq5VDNGVUREZCYYRqhosbHA339r7kMTHl7iZrSzaNhFQ0REz2IYoaJpr4q0basZM1ICiemJOHzrMADg1foMI0REpI9hhIqmvUtvKbtoBARa+LZADbcaxqmLiIjMBsMIFS4tDTh4ULNdwvVFsnKysODoAgBAvwb9jFQYERGZE4YRKty+fUBODlCrFlCnToma+PTYp4h7HIdqLtUwpsUYIxdIRETmgGGECpe3i0YmM/jpt1NuI/JIJADgky6fwNHW0ZjVERGRmWAYoYKp1cCOHZrtEnbRvL/vfWTlZqFd9XbsoiEiokIxjFDBTp4EkpMBFxfNTBoDHYo7hI1/b4SVzArLui2DrARXVoiIyDIwjFDBtF00L70E2Noa9NRcdS7G7R4HABjdbDSCvYONXBwREZkThhEqWCnu0rvm9BpcSLwAdzt3zO4028iFERGRuWEYofzi44Hz5wErK839aAzwT+Y/mBEzAwAwu+NseDh4lEWFRERkRhhGKD/tVZHQUMDDsDAxM2YmHmY9RKMqjfBmizfLoDgiIjI3DCOUXwm7aM4nnMfq06sBAMu6LYO1Vcnu8EtERJaFYYT0ZWQA0dGabQPCiBAC43aPg1qo8Vr919ChRoeyqY+IiMwOwwjpi44GsrOBGjWABg2K/bQfL/6Iw7cOw97aHp90+aTs6iMiIrPDMEL68nbRFHNtkAxlBibvnQwAmNJ2Cvzd/MuqOiIiMkMMI/SUECUaL7Lg6ALEp8bD39Uf77V+r4yKIyIic8UwQk+dOQPcvw84OgIdOhTrKbGPYrHw6EIAwKKui2BvY1+GBRIRkTliGKGntFdFunYFFIpiPWXyvsnIVmWjU0An9K7XuwyLIyIic8UwQk8Z2EWz/+Z+bL20FXKZHEtfWsr7zxARUYkwjJDGvXvAqVOa7e7dn3t4jioH43ePBwD8t+V/0bBKw7KsjoiIzBjDCGns3Kn53KoV4OX13MM/O/UZLiZfRGX7yviow0dlXBwREZkzhhHSMKCLJjkjGTNjZgIA5nWeB3d797KsjIiIzBzDCAFPngD79mm2IyKee/j0A9ORkp2Cpt5NMaLpiDIujoiIzB3DCAExMUBmJlC1KtCkSZGHnr53Gl+e+RKA5v4zcit5eVRIRERmjGGEir3qqvb+MwICrzd6HW2rty2nAomIyJwxjFg6IYBff9VsP6eLZv2f63Es/hgcbRyxMGxhORRHRESWgGHE0v35JxAfD9jbA506FXpYWnYa3tunWep9WrtpqOpStbwqJCIiM8cwYum0XTRhYZpAUoh5v83D/fT7qOleE5NCJ5VTcUREZAkYRiydtoumiCm91x9ex+LfFwMAloQvgZ21XXlURkREFoJhxJIlJQF//KHZLmLV1Ul7JkGpUiI8MBwRdZ4/9ZeIiMgQDCOWbNcuzQDWZs0003oLOuTaLvx69VdYW1kj6qUo3n+GiIiMrkRhZOXKlahRowbs7OwQEhKCEydOFHrsF198gXbt2sHd3R3u7u4ICwsr8ngqR8/polGqlJiwZwIAYFyrcQjyCCqnwoiIyJIYHEY2btyISZMmYdasWThz5gyaNGmC8PBwJCUlFXj8wYMHMWDAAMTExOD48ePw8/ND165dcffu3VIXT6WgVAJ79mi2Cwkjy/9Yjqv/XEUVxyqY2X5mORZHRESWRCaEEIY8ISQkBC1btsSKFSsAAGq1Gn5+fnjnnXcwZcqU5z5fpVLB3d0dK1aswJAhQ4p1ztTUVLi6uiIlJQUuLi6GlEuF2b8f6NIF8PYG7t4FrPRzaUJ6Auosr4M0ZRq+7vk1hjcdLlGhRERkqor7+9ugKyNKpRKnT59GWFjY0wasrBAWFobjx48Xq43MzEzk5OSgUqVKhR6TnZ2N1NRUvQ8yMm0XTffu+YIIAEyNnoo0ZRpa+rbE0OCh5VwcERFZEoPCyIMHD6BSqeD1zC3mvby8kJCQUKw2PvjgA/j6+uoFmmdFRkbC1dVV9+Hn52dImfQ8eVddLaCL5o87f2DduXUANPefsZJxnDMREZWdcv0tM3/+fGzYsAE//fQT7OwKX6ti6tSpSElJ0X3Ex8eXY5UW4PJlIDYWUCg0i53loRZqjNs9DgAwtMlQvFDtBSkqJCIiC2JtyMEeHh6Qy+VITEzU25+YmAhvb+8in/vpp59i/vz52L9/Pxo3blzksQqFAgqFwpDSyBDaqyIdOwJOTnoPfXv+W5y4ewLOts6I7BwpQXFERGRpDLoyYmtri+bNmyM6Olq3T61WIzo6GqGhoYU+b+HChZg9ezZ2796NFi1alLxaMo68d+nNIzU7FVP2awYhz3hxBnycfcq7MiIiskAGXRkBgEmTJmHo0KFo0aIFWrVqhaioKGRkZGD4cM1siyFDhqBq1aqIjNT8Vb1gwQLMnDkT69evR40aNXRjS5ycnOD0zF/lVA4ePgSOHtVsPxNGZh+ajcSMRNSpXAfjXxgvQXFERGSJDA4j/fr1Q3JyMmbOnImEhAQEBwdj9+7dukGtt2/fhlWe2RmfffYZlEol+vTpo9fOrFmz8OGHH5auejLcrl2AWg00agT4++t2X35wGVF/RAEAosKjYCu3lahAIiKyNAaHEQB4++238fbbbxf42MGDB/W+jouLK8kpqKwU0EUjhMCE3ROQq85Fjzo90K12N4mKIyIiS8Q5m5YkJwfYvVuzHfH0hnfbr27Hnht7YGNlg8VdF0tUHBERWSqGEUty9Cjw+DHg4QG0agUAyM7NxsQ9EwEAk0InoXbl2hIWSERElohhxJJou2hefhmQywEAS35fghuPbsDHyQfT202XsDgiIrJUDCOWRBtG/u2iuZt6F3MOzwEALOyyEM4KZ6kqIyIiC8YwYimuXQOuXAGsrYGuXQEAH+z/ABk5GQitFoqBjQZKXCAREVkqhhFLob0q0r494OKCo7eP4oc/f4AMMizrtgwymUza+oiIyGIxjFgK7RLwERFQqVW6+8+MaDoCLXy5Ki4REUmHYcQSPH4M/PabZrtHD3x99mucuX8GrgpXzO08V9LSiIiIGEYswd69QG4uEBSER76VMO3ANADAhx0+RBXHKhIXR0RElo5hxBLk6aL56NBHeJD5APU86mFsy7HS1kVERIQSLgdPJkSlAnbuBAD83bEBVpwYAQBY+tJS2MhtpKyMiIgIAK+MmL/ffwcePoRwd8P4pG+hEir0CuqFLoFdpK6MiIgIAMOI+fu3i2bba40QHXcACrkCi7oukrgoIiKipxhGzN327ciyBiZVvwwAeK/1e6jpXlPiooiIiJ5iGDFnsbHA33/j07YyxOUmo5pLNUxpO0XqqoiIiPQwjJiz7dtx2xWIbKdZXfWTLp/A0dZR4qKIiIj0MYyYs+3b8X4XIEuuRrvq7dCvQT+pKyIiIsqHYcRcpaXh0M0D2NgQsJJZ8f4zRERUYTGMmKncvbsxLiwXAPBm89EI9g6WtiAiIqJCMIyYqTWHFuOCN+CuVmB2xzlSl0NERFQohhEz9E96MmY4/gEAmF1rNCo7VJa4IiIiosIxjJihmT+OwUM7gUbJVnjztflSl0NERFQkhhEzcz7hPFbf2goAWJbeDtZ2DhJXREREVDSGETMihMC43eOglgm89jfQodMbUpdERET0XAwjZuTHiz/i8K3DsM8BPtkH4OWXpS6JiIjouRhGzESGMgOT904GAEw5AvjXbw14eEhcFRER0fMxjJiJBUcXID41Hv7Z9njvKIAePaQuiYiIqFgYRsxA7KNYLDy6EACwaGcu7HMBRERIWxQREVExMYyYgcn7JiNblY1Ozo3R+3wO4O8PNGggdVlERETFwjBi4vbf3I+tl7ZCLpNj6Y06kAGaLhreh4aIiEwEw4gJy1HlYPzu8QCAsS3/i4bbjmkeYBcNERGZEIYRE/bZqc9wMfkiKttXxofuvYF79wBHR6B9e6lLIyIiKjaGEROVnJGMmTEzAQDzOs+D+97Dmge6dAHs7CSsjIiIyDAMIyZq+oHpSMlOQVPvphjRdATw66+aB9hFQ0REJoZhxASdvncaX575EgCwrNsyyBOTgFOnNA9y1VUiIjIxDCMmRnv/GQGB1xu9jrbV2wI7d2oebNkS8PaWtkAiIiIDMYyYmPV/rsex+GNwtHHEwjDNQmfsoiEiIlPGMGJC1EKNjw9/DACY3m46qrpUBZ48Afbt0xzAJeCJiMgEMYyYECuZFWKGxmDiCxMxMXSiZufBg0BmJlC1KhAcLGV5REREJWItdQFkGF9nXywOX/x0h7aLhquuEhGRieKVEVMmBLB9u2abXTRERGSiGEZM2V9/AbdvaxY569RJ6mqIiIhKhGHElGm7aMLCAAcHaWshIiIqIYYRU8YuGiIiMgMMI6YqORn4/XfNdvfu0tZCRERUCgwjpmrnTs0A1qZNgWrVpK6GiIioxBhGTBW7aIiIyEwwjJgipRLYs0ezzTBCREQmjmHEFB0+DKSlAV5eQIsWUldDRERUKgwjpkjbRdO9O2DFf0IiIjJt/E1maoTQXwKeiIjIxDGMmJrLl4GbNwFbW6BLF6mrISIiKjXeKK+iU6uBu3c1AeTGjadXRTp2BJycpK2NiIjICBhGKoKMDCA2VhM2bt58Gjxu3tTsVyrzP+eVV8q/TiIiojLAMFIehAASEvRDRt7gkZBQ9POtrQF/fyAwEKhZE2jYEBgxonxqJyIiKmMMI8by5AkQF1fw1Y2bN4GsrKKf7+b2NGxoP2u3q1XTBBIiIiIzVKLfcCtXrsQnn3yChIQENGnSBMuXL0erVq0KPf7HH3/EjBkzEBcXh9q1a2PBggV4+eWXS1y0JIQAHjwoOGzcuKEZ11EUKyugevWCw0bNmoC7e/m8DiIiogrG4DCyceNGTJo0CatXr0ZISAiioqIQHh6OK1euoEqVKvmOP3bsGAYMGIDIyEj06NED69evR69evXDmzBk0bNjQKC/CaJRK4Natgq9s3LgBpKcX/XwnJ024KChs+PsDNjbl8zqIiIhMiEwIIQx5QkhICFq2bIkVK1YAANRqNfz8/PDOO+9gypQp+Y7v168fMjIysF27UBeAF154AcHBwVi9enWxzpmamgpXV1ekpKTAxcXFkHKLtmIFcP7807ARH6+ZvVIYmQyoWrXw7pTKlTXHEBERUbF/fxt0ZUSpVOL06dOYOnWqbp+VlRXCwsJw/PjxAp9z/PhxTJo0SW9feHg4tm3bVuh5srOzkZ2drfs6NTXVkDKL7/vvgT/+0N/n4JD/qoZ2298fsLMrm1qIiIgslEFh5MGDB1CpVPDy8tLb7+XlhcuXLxf4nISEhAKPTyhiBklkZCQ++ugjQ0ormaFDgW7d9IOHlxevbhAREZWjCjlFY+rUqXpXU1JTU+Hn52f8E40ZY/w2iYiIyCAGhREPDw/I5XIkJibq7U9MTIS3t3eBz/H29jboeABQKBRQKBSGlEZEREQmyqB709ja2qJ58+aIjo7W7VOr1YiOjkZoaGiBzwkNDdU7HgD27dtX6PFERERkWQzuppk0aRKGDh2KFi1aoFWrVoiKikJGRgaGDx8OABgyZAiqVq2KyMhIAMD48ePRvn17LFq0CN27d8eGDRtw6tQprFmzxrivhIiIiEySwWGkX79+SE5OxsyZM5GQkIDg4GDs3r1bN0j19u3bsLJ6esGldevWWL9+Pf7v//4P06ZNQ+3atbFt27aKt8YIERERScLgdUakUGbrjBAREVGZKe7vb4PGjBAREREZG8MIERERSYphhIiIiCTFMEJERESSYhghIiIiSTGMEBERkaQYRoiIiEhSDCNEREQkqQp5195naddlS01NlbgSIiIiKi7t7+3nra9qEmEkLS0NAODn5ydxJURERGSotLQ0uLq6Fvq4SSwHr1arce/ePTg7O0Mmk0ldjqRSU1Ph5+eH+Ph4Lo1fxvhelw++z+WD73P54PusTwiBtLQ0+Pr66t237lkmcWXEysoK1apVk7qMCsXFxYXf6OWE73X54PtcPvg+lw++z08VdUVEiwNYiYiISFIMI0RERCQphhETo1AoMGvWLCgUCqlLMXt8r8sH3+fywfe5fPB9LhmTGMBKRERE5otXRoiIiEhSDCNEREQkKYYRIiIikhTDCBEREUmKYYSIiIgkxTBiIiIjI9GyZUs4OzujSpUq6NWrF65cuSJ1WWZv/vz5kMlkmDBhgtSlmJ27d+9i0KBBqFy5Muzt7dGoUSOcOnVK6rLMikqlwowZMxAQEAB7e3sEBgZi9uzZz71pGT3f4cOHERERAV9fX8hkMmzbtk3vcSEEZs6cCR8fH9jb2yMsLAzXrl2TplgTwDBiIg4dOoSxY8fi999/x759+5CTk4OuXbsiIyND6tLM1smTJ/H555+jcePGUpdidh49eoQ2bdrAxsYGu3btwsWLF7Fo0SK4u7tLXZpZWbBgAT777DOsWLECly5dwoIFC7Bw4UIsX75c6tJMXkZGBpo0aYKVK1cW+PjChQuxbNkyrF69Gn/88QccHR0RHh6OJ0+elHOlpoHrjJio5ORkVKlSBYcOHcKLL74odTlmJz09Hc2aNcOqVaswZ84cBAcHIyoqSuqyzMaUKVNw9OhR/Pbbb1KXYtZ69OgBLy8vfPXVV7p9r776Kuzt7fH9999LWJl5kclk+Omnn9CrVy8Amqsivr6+ePfddzF58mQAQEpKCry8vLBu3Tr0799fwmorJl4ZMVEpKSkAgEqVKklciXkaO3YsunfvjrCwMKlLMUu//PILWrRogddeew1VqlRB06ZN8cUXX0hdltlp3bo1oqOjcfXqVQDA+fPnceTIEXTr1k3iysxbbGwsEhIS9H5+uLq6IiQkBMePH5ewsorLJO7aS/rUajUmTJiANm3aoGHDhlKXY3Y2bNiAM2fO4OTJk1KXYrZu3ryJzz77DJMmTcK0adNw8uRJjBs3Dra2thg6dKjU5ZmNKVOmIDU1FUFBQZDL5VCpVJg7dy4GDhwodWlmLSEhAQDg5eWlt9/Ly0v3GOljGDFBY8eOxV9//YUjR45IXYrZiY+Px/jx47Fv3z7Y2dlJXY7ZUqvVaNGiBebNmwcAaNq0Kf766y+sXr2aYcSINm3ahB9++AHr169HgwYNcO7cOUyYMAG+vr58n6lCYTeNiXn77bexfft2xMTEoFq1alKXY3ZOnz6NpKQkNGvWDNbW1rC2tsahQ4ewbNkyWFtbQ6VSSV2iWfDx8UH9+vX19tWrVw+3b9+WqCLz9N5772HKlCno378/GjVqhMGDB2PixImIjIyUujSz5u3tDQBITEzU25+YmKh7jPQxjJgIIQTefvtt/PTTTzhw4AACAgKkLsksde7cGX/++SfOnTun+2jRogUGDhyIc+fOQS6XS12iWWjTpk2+qelXr16Fv7+/RBWZp8zMTFhZ6f+Yl8vlUKvVElVkGQICAuDt7Y3o6GjdvtTUVPzxxx8IDQ2VsLKKi900JmLs2LFYv349fv75Zzg7O+v6HV1dXWFvby9xdebD2dk53zgcR0dHVK5cmeNzjGjixIlo3bo15s2bh759++LEiRNYs2YN1qxZI3VpZiUiIgJz585F9erV0aBBA5w9exaLFy/GG2+8IXVpJi89PR3Xr1/XfR0bG4tz586hUqVKqF69OiZMmIA5c+agdu3aCAgIwIwZM+Dr66ubcUPPEGQSABT4sXbtWqlLM3vt27cX48ePl7oMs/Prr7+Khg0bCoVCIYKCgsSaNWukLsnspKamivHjx4vq1asLOzs7UbNmTTF9+nSRnZ0tdWkmLyYmpsCfyUOHDhVCCKFWq8WMGTOEl5eXUCgUonPnzuLKlSvSFl2BcZ0RIiIikhTHjBAREZGkGEaIiIhIUgwjREREJCmGESIiIpIUwwgRERFJimGEiIiIJMUwQkRERJJiGCEiIiJJMYwQERGRpBhGiIiISFIMI0RERCSp/weotUuY5X5GtQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def len_dis(text):\n",
        "  lens = [len(line) for line in text]\n",
        "  len_counter = collections.Counter(lens)\n",
        "\n",
        "  lens = np.array(list(len_counter.keys()))\n",
        "  sort_idx = np.argsort(lens)\n",
        "  lens_sort = lens[sort_idx]\n",
        "  len_counts = np.array(list(len_counter.values()))\n",
        "  len_counts_sort = len_counts[sort_idx]\n",
        "  p = np.cumsum(len_counts_sort) / len_counts_sort.sum()\n",
        "  return p, lens_sort\n",
        "\n",
        "src_p, src_lens_sort = len_dis(source)\n",
        "tgt_p, tgt_lens_sort = len_dis(target)\n",
        "plt.plot(src_lens_sort, src_p, 'r-', label='eng')\n",
        "plt.plot(tgt_lens_sort, tgt_p, 'g-', label='fra')\n",
        "plt.title('Cumulative Distribution of Sentence Length')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjqvL3LsFPJ7"
      },
      "source": [
        "From the above plots, we can see that more than 90% of the sentences have a length of less than 8. Thus, we can filter out sentences of length greater than 8. We also filter out words that occur less than 5 times in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1Q_UrZKyFPJ7"
      },
      "outputs": [],
      "source": [
        "# hyper-param\n",
        "MAX_LEN = 8\n",
        "MIN_FREQ = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSK-nzTiFPJ7"
      },
      "source": [
        "### Build Vocabulary\n",
        "\n",
        "Each word needs a unique index, and the words that have been filtered out need a special token to represent them. The following class Vocab is used to build the vocabulary. Some basic helper functions or dictionaries are also provided:\n",
        "- Dictionary word2index: Convert word string into index:\n",
        "- Dictionary index2word: Convert index into word string\n",
        "- helper function _build_vocab(): Build dictionaries for converting from words to indices and vice versa\n",
        "- Word Counter, num_word: Record the total number of unique tokens in the vocabulary\n",
        "    \n",
        "There are 4 special tokens added in the vocabulary:\n",
        "- 'pad': padding token. Sentences shorter than MAX_LEN is padded by this symbol to make the length to MAX_LEN\n",
        "- 'bos': beginning of sentence. This indicates the beginning of a sentence\n",
        "- 'eos': end of sentence. This indicates the end of a sentence\n",
        "- 'unk': unknown word. This represents words that have been filtered out (words that are not in the vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OWUntlMGFPJ7"
      },
      "outputs": [],
      "source": [
        "class Vocab():\n",
        "  def __init__(self, name, tokens, min_freq):\n",
        "    self.name = name\n",
        "    self.index2word = {\n",
        "      0: 'pad',\n",
        "      1: 'bos',\n",
        "      2: 'eos',\n",
        "      3: 'unk'\n",
        "    }\n",
        "    self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "    self.num_word = 4\n",
        "    token_freq = collections.Counter(tokens)\n",
        "    tokens = [token for token in tokens if token_freq[token] >= MIN_FREQ]\n",
        "    self._build_vocab(tokens)\n",
        "\n",
        "  def _build_vocab(self, tokens):\n",
        "    for token in tokens:\n",
        "      if token not in self.word2index:\n",
        "        self.word2index[token] = self.num_word\n",
        "        self.index2word[self.num_word] = token\n",
        "        self.num_word += 1\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.word2index.get(tokens, self.word2index['unk'])\n",
        "    else:\n",
        "      return [self.__getitem__(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StRHfRCWFPJ8"
      },
      "source": [
        "### Build Dataset\n",
        "\n",
        "The dataset pipeline involves the following steps:\n",
        "- For target language, every sentence will be 'sandwiched' with the 'bos' token and the 'eos' token.\n",
        "- Every sentence that has a length less than MAX_LEN will be padded to the MAX_LEN with the *padding_token*.\n",
        "- The dataset should return the converted tensor and the corresponding valid length before padding.\n",
        "- We use the Pytorch *DataLoader* API to build the dataset generator.\n",
        "\n",
        "For the purposes of this assignment, we will train and evaluate on only the training data. This isn't ideal because we do not know if we are  overfitting to the training data, but it is fine for instructional purposes. In practice (eg. for your projects), you should make sure to split your data into training/validation/test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dbLD4AS5FPJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442895fc-a9e5-402f-fae8-a3940a404e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "Vocabulary size of source language: 433\n",
            "Vocabulary size of target language: 420\n",
            "Total number of sentence pairs: 4990\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(name, tokens, min_freq):\n",
        "  tokens = [token for line in tokens for token in line]\n",
        "  return Vocab(name, tokens, min_freq)\n",
        "\n",
        "def build_vocabs(lang_src, lang_tgt, src_text, tgt_text):\n",
        "  vocab_src = build_vocab(lang_src, src_text, MIN_FREQ)\n",
        "  vocab_tgt = build_vocab(lang_tgt, tgt_text, MIN_FREQ)\n",
        "  return vocab_src, vocab_tgt\n",
        "\n",
        "def pad(line, padding_token):\n",
        "  return line + [padding_token] * (MAX_LEN + 2 - len(line))\n",
        "\n",
        "def build_tensor(text, lang, is_source):\n",
        "  lines = [lang[line] for line in text]\n",
        "  if not is_source:\n",
        "    lines = [[lang['bos']] + line + [lang['eos']] for line in lines]\n",
        "  array = torch.tensor([pad(line, lang['pad']) for line in lines])\n",
        "  valid_len = (array != lang['pad']).sum(1)\n",
        "  return array, valid_len\n",
        "\n",
        "def load_data_nmt(batch_size=2):\n",
        "  lang_eng, lang_fra = build_vocabs('eng', 'fra', source, target)\n",
        "  src_array, src_valid_len = build_tensor(source, lang_eng, True)\n",
        "  tgt_array, tgt_valid_len = build_tensor(target, lang_fra, False)\n",
        "  train_data = torch.utils.data.TensorDataset(\n",
        "    src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "  print(train_data[0])\n",
        "  train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
        "  return lang_eng, lang_fra, train_iter\n",
        "\n",
        "\n",
        "source, target = prepare_data(raw_text, max_len=MAX_LEN)\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size=2)\n",
        "print('Vocabulary size of source language: {}'.format(vocab_eng.num_word))\n",
        "print('Vocabulary size of target language: {}'.format(vocab_fra.num_word))\n",
        "print('Total number of sentence pairs: {}'.format(len(source)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwqzJmZhFPJ9"
      },
      "source": [
        "## Sequence to Sequence with RNN (baseline)\n",
        "\n",
        "In this section, we provide the implementation of the seq2seq RNN baseline model. You do not need to implement any code in this section, but you should read and understand what the code is doing because you will need to implement something similar in subsequent sections. The following figure highlights the architecture of the seq2seq model. An encoder RNN encodes the input sequence into its hidden state, and passes the last hidden state to the decoder RNN. The decoder generates the target sequence.\n",
        "\n",
        "Implementation Details:\n",
        "\n",
        "- Embedding: We have represented each word with an integer or one-hot vector. We need an embedding layer to map an input word to its embedding vector.\n",
        "- Encoder: A vanilla RNN is used to encode a source sequence. The final hidden state is returned as output and passed to the decoder RNN.\n",
        "- Decoder: Another vanilla RNN is implemented to generate the target sequence. The hidden state is initialized with the last hidden state from the encoder.\n",
        "- Encoder-Decoder: The class NMTRNN is built by combining the encoder and the decoder, and yields the loss and predictions.\n",
        "- Loss: We have padded all sentences so that they have the same MAX_LEN. Thus, when we compute the loss, the loss from those padding_tokens should be masked out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3vWXY06FPJ9"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/24e89824c154c2afc419c5dadec9622e490b99bb/img/seq2seq.svg\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://github.com/dsgiitr/d2l-pytorch/blob/master/img/seq2seq.svg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cSVu0r5OFPJ9"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim) # embedding layer\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, sources, valid_len):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      source: tensor of size (N, T), where N is the batch size, T is the length of the sequence(s)\n",
        "      valid_len: tensor of size (N,), indicating the valid length of sequence(s) (the length before padding)\n",
        "    \"\"\"\n",
        "    word_embedded = self.embedding(sources)\n",
        "    N = word_embedded.shape[0]\n",
        "\n",
        "    h = sources.new_zeros(1, N, self.hidden_size).float() # initialize hidden state with zeros\n",
        "\n",
        "    o, h = self.enc(word_embedded, h)\n",
        "\n",
        "    return o[np.arange(N), valid_len] # return the hidden state of the valid last time step\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of the hidden state of vanilla RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, h, target):\n",
        "    word_embedded = self.embedding(target)\n",
        "    N, T = word_embedded.shape[:2]\n",
        "\n",
        "    o, h = self.enc(word_embedded, h.view(1,N,self.hidden_size))\n",
        "    pred = self.output_emb(o)\n",
        "    return pred, h\n",
        "\n",
        "class NMTRNN(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size):\n",
        "    super(NMTRNN, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size)\n",
        "\n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    h = self.enc(src, src_len)\n",
        "    T = tgt.shape[1]\n",
        "\n",
        "    pred, _ = self.dec(h, tgt)\n",
        "    loss = 0\n",
        "    for t in range(T-1):\n",
        "      # target sequence should shift by one time-step, because we are predicting the next word\n",
        "      # Note that in principle the `ignore_index` parameter can be set to 0, which is for the `pad` token.\n",
        "      # Due to version change in PyTorch, this may have a bug. Therefore we do not use it here.\n",
        "      loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1]) #, ignore_index=0)\n",
        "\n",
        "    return loss, pred.argmax(dim=-1)\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "      \"\"\"\n",
        "      When predicting a sequence given the 'bos' token, the input for the next step is the predicted\n",
        "      token from the previous time step.\n",
        "      \"\"\"\n",
        "      h = self.enc(src, src_len)\n",
        "\n",
        "      inputs = tgt[:, :1]\n",
        "      preds = []\n",
        "      for t in range(MAX_LEN+1): # plus the 'eos' token\n",
        "        pred, h = self.dec(h, inputs)\n",
        "        preds.append(pred)\n",
        "        inputs = pred.argmax(dim=-1)\n",
        "\n",
        "      pred = torch.cat(preds, dim=1).argmax(dim=-1)\n",
        "      return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tKR2Lpb8FPJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11d8766-ce39-4b21-c5bc-c721e0f107e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-a657ea165a45>:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss = loss + F.nll_loss(F.log_softmax(pred[:, t]), tgt[:, t+1]) #, ignore_index=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 / 7800\tLoss:\t55.998413\n",
            "pred:\t tensor([146,  65, 349,  53,   3, 330,  52, 330, 330, 330])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t10.684528\n",
            "pred:\t tensor([14,  3,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([15, 89,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t9.897626\n",
            "pred:\t tensor([  3, 164,   3,  11,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 48,  49, 113, 282,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t8.474100\n",
            "pred:\t tensor([3, 3, 5, 3, 5, 2, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([36,  3, 37,  3,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t6.623552\n",
            "pred:\t tensor([36,  3,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([36,  9, 88, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t6.568908\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t7.220339\n",
            "pred:\t tensor([ 14,  28,   3, 227,  37,   3,  11,   2,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14,  28,  35, 227,  37,   3,  11,   2,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t5.931001\n",
            "pred:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t6.431123\n",
            "pred:\t tensor([ 14, 116,  72,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 197,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t5.054590\n",
            "pred:\t tensor([ 14, 171,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([14, 70,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t6.022000\n",
            "pred:\t tensor([190,  24,  24,   3,  41,  24,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t5.763693\n",
            "pred:\t tensor([ 14, 171,   3, 205,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14,  70,  17, 205,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t4.050896\n",
            "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t5.096206\n",
            "pred:\t tensor([ 14, 171,   3,  75,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 171, 213,  75,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t4.131536\n",
            "pred:\t tensor([176, 203,  14, 171,   3,  24,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([176, 203,  14, 116,   3,  24,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t4.293114\n",
            "pred:\t tensor([ 14, 385,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([103, 385,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t4.409428\n",
            "pred:\t tensor([15,  3, 72,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([ 15, 291,   9,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t4.500104\n",
            "pred:\t tensor([ 14, 116, 179, 259,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 305,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t3.820551\n",
            "pred:\t tensor([135, 316,  41, 139,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([315, 316,  41, 139,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t2.989162\n",
            "pred:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t3.299886\n",
            "pred:\t tensor([ 14, 265,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3, 265,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t3.311011\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t3.245933\n",
            "pred:\t tensor([  3,   6, 248,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3,  72, 248,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t3.633176\n",
            "pred:\t tensor([176,   3, 302,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3,  37, 355,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t3.428319\n",
            "pred:\t tensor([ 14, 197,  74,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 289,  74,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t2.871987\n",
            "pred:\t tensor([176,   3, 237,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([176,   9, 237,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t3.465694\n",
            "pred:\t tensor([ 38, 116, 203,  14,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 38, 419, 203,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t3.260435\n",
            "pred:\t tensor([ 90, 170,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 90, 176,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t2.741233\n",
            "pred:\t tensor([  3, 207,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3, 207,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t2.730143\n",
            "pred:\t tensor([251,  27,   5,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t2.468782\n",
            "pred:\t tensor([  3, 314,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3, 314,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t2.438234\n",
            "pred:\t tensor([ 38,  40, 102, 390,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 38,  40,  21, 390,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t3.271469\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t3.184861\n",
            "pred:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([267, 234,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t2.631399\n",
            "pred:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t2.183910\n",
            "pred:\t tensor([36, 40,  3,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([ 38,  78, 214,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t2.369658\n",
            "pred:\t tensor([48,  3, 11, 10, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([48,  3, 37, 10, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t1.841551\n",
            "pred:\t tensor([  3, 161,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([  3, 161,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t2.051502\n",
            "pred:\t tensor([3, 5, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([ 3, 19,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t2.307741\n",
            "pred:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t1.966699\n",
            "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t2.471095\n",
            "pred:\t tensor([171, 341, 236,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 92, 341, 236,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t2.270563\n",
            "pred:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([267, 118,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t2.341895\n",
            "pred:\t tensor([ 14,   3, 114,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14,  17, 114,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t1.205045\n",
            "pred:\t tensor([ 14,  79,  28,  41, 151,  11,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14,  79,  28,  41, 229,  11,   2,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t2.454021\n",
            "pred:\t tensor([267,   3,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([267, 124,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.839891\n",
            "pred:\t tensor([ 15, 204, 393,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 15, 204, 393,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t1.673559\n",
            "pred:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t2.115613\n",
            "pred:\t tensor([15,  3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([15, 23, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t1.782002\n",
            "pred:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_rnn(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "rnn_net = NMTRNN(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size)\n",
        "\n",
        "rnn_loss_list = train_rnn(rnn_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv_d48GoFPJ-"
      },
      "source": [
        "### RNN Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gj1l0_J0FPJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "468e53c8-c00e-41e6-887f-072bba61e020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Baseline')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# save the loss curve figure in a file for the report\n",
        "if device != \"cpu\":\n",
        "    rnn_loss_list_cpu = [ele.cpu() for ele in rnn_loss_list]\n",
        "else:\n",
        "    rnn_loss_list_cpu = run_loss_list\n",
        "plt.plot(np.arange(len(rnn_loss_list_cpu)), rnn_loss_list_cpu)\n",
        "plt.title('Loss Curve of Baseline')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyIwLicjFPJ-"
      },
      "source": [
        "### Prediction Accuracy\n",
        "\n",
        "Print out 5 prediction samples, and calculate the prediction accuracy over the training dataset. You will see an accuracy of over 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Rv40njI-FPJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a97848e-55bd-41e1-cb4d-9f94988f6f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['unk', '!', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'unk', 'y', 'aller', '.', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'grand', '?', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['unk', 'à', 'qui', 'que', 'ce', 'soit', '!', 'pad']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.8841\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "\n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "\n",
        "def evaluate_rnn(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()]\n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_rnn(rnn_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigkvZsTFPJ_"
      },
      "source": [
        "## Sequence to Sequence with LSTM and Attention\n",
        "\n",
        "Now let's try to improve our model by using an LSTM and the attention mechanism.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imBxyLCJFPKA"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "LSTMs eliminate the gradient explosion/vanishing problem. Its state and gate update at each time step can be summarized as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "&\\text{State Update} &&& C_t &= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\\n",
        "&\\text{Hidden States} &&& H_t &= O_t \\odot \\text{tanh}(C_t) \\\\\n",
        "&\\text{Proposal} &&& \\tilde{C}_t &= \\text{tanh}( X_tW_{xc} + H_{t-1}W_{hc} + b_c ) \\\\\n",
        "&\\text{Input Gate} &&& I_t &= \\sigma( X_tW_{xi} + H_{t-1}W_{hi} + b_i ) \\\\\n",
        "&\\text{Forget Gate} &&& F_t &= \\sigma( X_tW_{xf} + H_{t-1}W_{hf} + b_f ) \\\\\n",
        "&\\text{Output Gate} &&& O_t &= \\sigma( X_tW_{xo} + H_{t-1}W_{ho} + b_o ) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Implement the LSTM class below. In particular,\n",
        "-  Complete the initialization function *init_params()*. Weights should be initialized using `torch.randn` multiplied with a scale of 0.1. Biases should be initialized to 0.\n",
        "- Complete the function *lstm()* which performs the feed-forward pass of LSTM. **Do not** use `nn.LSTM` or `nn.LSTMCell` in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "id": "YG8YK1JCFPKA",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "386a51799571153e76849c4b5ebb7f73",
          "grade": false,
          "grade_id": "cell-e43516618029ca06",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, device):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.device = device\n",
        "    self.params = nn.ParameterList(self.init_params(input_size, hidden_size))\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "      device: torch.device()\n",
        "    \"\"\"\n",
        "\n",
        "  def init_params(self, input_size, hidden_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of input sequence\n",
        "      hidden_size: int, feature dimension of hidden state\n",
        "\n",
        "    Outputs:\n",
        "      Weights for proposal: W_xc, W_hc, b_c\n",
        "      Weights for input gate: W_xi, W_hi, b_i\n",
        "      Weights for forget gate: W_xf, W_hf, b_f\n",
        "      Weights for output gate: W_xo, W_ho, b_o\n",
        "    \"\"\"\n",
        "    W_xc, W_hc, b_c = None, None, None\n",
        "    W_xi, W_hi, b_i = None, None, None\n",
        "    W_xf, W_hf, b_f = None, None, None\n",
        "    W_xo, W_ho, b_o = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the weights and biases. The result will be stored in\n",
        "    # `params` below. Weights should be initialized using `torch.randn` multiplied\n",
        "    # with the scale (0.1). Biases should be initialized to 0.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    scale = 0.1\n",
        "    W_xc = nn.Parameter(torch.randn(input_size, hidden_size) * scale)\n",
        "    W_hc = nn.Parameter(torch.randn(hidden_size, hidden_size) * scale)\n",
        "    b_c = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    W_xi = nn.Parameter(torch.randn(input_size, hidden_size) * scale)\n",
        "    W_hi = nn.Parameter(torch.randn(hidden_size, hidden_size) * scale)\n",
        "    b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    W_xf = nn.Parameter(torch.randn(input_size, hidden_size) * scale)\n",
        "    W_hf = nn.Parameter(torch.randn(hidden_size, hidden_size) * scale)\n",
        "    b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    W_xo = nn.Parameter(torch.randn(input_size, hidden_size) * scale)\n",
        "    W_ho = nn.Parameter(torch.randn(hidden_size, hidden_size) * scale)\n",
        "    b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    params = [W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o]\n",
        "    return params\n",
        "\n",
        "\n",
        "  def lstm(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tuple of tensors (src, src_len). src, size (N, D_in) or (N, T, D_in), where N is the batch size,\n",
        "        T is the length of the sequence(s). src_len, size of (N,), is the valid length for each sequence.\n",
        "\n",
        "      state: tuple of tensors (h, c). h, size of (N, hidden_size) is the hidden state of LSTM. c, size of\n",
        "            (N, hidden_size), is the memory cell of the LSTM.\n",
        "\n",
        "    Outputs:\n",
        "      o: tensor of size (N, T, hidden_size). Contains the output features (the hidden state H_t) for each t.\n",
        "      state: the same as input state. Contains the hidden state H_T and cell state C_T for the last timestep T.\n",
        "    \"\"\"\n",
        "\n",
        "    src, src_len = X\n",
        "    h, c = state\n",
        "\n",
        "    # make sure always has a T dim\n",
        "    if len(src.shape) == 2:\n",
        "      src = src.unsqueeze(1)\n",
        "\n",
        "    N, T, D_in = src.shape\n",
        "    W_xc, W_hc, b_c, W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o = self.params\n",
        "    o = []\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of the LSTM.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    for t in range(T):\n",
        "        xt = src[:, t, :]\n",
        "        c_tilde = torch.tanh(xt @ W_xc + h @ W_hc + b_c)\n",
        "        i_t = torch.sigmoid(xt @ W_xi + h @ W_hi + b_i)\n",
        "        f_t = torch.sigmoid(xt @ W_xf + h @ W_hf + b_f)\n",
        "        o_t = torch.sigmoid(xt @ W_xo + h @ W_ho + b_o)\n",
        "        c = f_t * c + i_t * c_tilde\n",
        "        h = o_t * torch.tanh(c)\n",
        "        o.append(h.unsqueeze(1))\n",
        "\n",
        "    o = torch.cat(o, dim=1)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    state = (h, c)\n",
        "    return o, state\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    return self.lstm(inputs, state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPgyFycTFPKB"
      },
      "source": [
        "Check that your output has the correct shape. You should see:\n",
        "\n",
        "```\n",
        "torch.Size([12, 8, 5])\n",
        "torch.Size([12, 5])\n",
        "torch.Size([12, 5])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zHlJbYXsFPKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81da03db-c945-4a02-a0b4-29eb1096b3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 8, 5])\n",
            "torch.Size([12, 5])\n",
            "torch.Size([12, 5])\n"
          ]
        }
      ],
      "source": [
        "test_lstm = LSTM(10, 5, torch.device('cpu'))\n",
        "test_src = torch.ones(12, 8, 10)\n",
        "test_src_len = torch.ones(12) * 8\n",
        "test_h = torch.zeros(12, 5).float()\n",
        "test_c = torch.zeros(12, 5).float()\n",
        "\n",
        "test_o, test_state = test_lstm((test_src, test_src_len), (test_h, test_c))\n",
        "\n",
        "print(test_o.shape)\n",
        "print(test_state[0].shape)\n",
        "print(test_state[1].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epm0XEhpFPKC"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1IcWv0FPKD"
      },
      "source": [
        "Another improvement we can make to our model is the Attention Mechanism. An example illustrating why applying attention mechanisms can improve the performance is shown in the picture below. An English sentence and its Chinese is visualized and aligned into blue boxes and red boxes, respectively. It can be seen that the Chinese character '她' has a long distance from its English counterpart, 'she'. Since only the final hidden state is passed to the decoder, it's hard for the baseline model to 'attend' to information a long time ago."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX-YAzgzFPKE"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png\" width=\"600\"/>\n",
        "</div>\n",
        "Image source: https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk5vQ9fGFPKE"
      },
      "source": [
        "- **Attention**\n",
        "\n",
        "    Given a query, $\\mathbf{q} \\in R^{d_q}$, and a set of $N$ (key, value) pairs, $\\{ \\mathbf{k}_i, \\mathbf{v}_i\\}^N$ where $k_i \\in R^{d_k}$ and $v_i \\in R^{d_v}$, the attention mechanism computes a weighted sum of values based on the normalized score obtained from the query and each key:\n",
        "    $$\n",
        "    \\begin{align*}\n",
        "    a_i &= \\alpha(\\mathbf{q}, \\mathbf{k_i}) \\\\\n",
        "    \\mathbf{a} &= [a_1, ..., a_n] \\\\\n",
        "    \\mathbf{b} &= \\text{softmax}(\\mathbf{a}) \\\\\n",
        "    \\mathbf{o} &= \\mathbf{b} \\cdot \\mathbf{V}\\text{, where } \\mathbf{V} = \\{\\mathbf{v}_i\\}^N\n",
        "    \\end{align*}\n",
        "    $$\n",
        "    The $\\alpha()$ function, which maps two vectors into a scalar, is the score function that can be chosen from a wide range of functions: e.g. the cosine function, dot-product function, scaled dot-product funtion and etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SvaotsZFPKE"
      },
      "source": [
        "- **Masked Softmax**\n",
        "\n",
        "For our machine translation task, the inputs and outputs may be of variable length (ie. each training example may have a different number of words). As shown above, we pad our inputs with a special `pad` token so that they all have the same length to make them easier to work with. However, when we take the softmax, we only want to include the non-`pad` items, so we need to write a special `masked_softmax` function to handle this. We can achieve the masking by setting masked elements to a large negative value. Then when we take the `exp`, those elements will be 0 and won't contribute to the softmax. We provide the implementation of this for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HMcgYib8FPKE"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(X, valid_length):\n",
        "  \"\"\"\n",
        "  inputs:\n",
        "    X: 3-D tensor\n",
        "    valid_length: 1-D or 2-D tensor\n",
        "  \"\"\"\n",
        "  mask_value = -1e7\n",
        "\n",
        "  if len(X.shape) == 2:\n",
        "    X = X.unsqueeze(1)\n",
        "\n",
        "  N, n, m = X.shape\n",
        "\n",
        "  if len(valid_length.shape) == 1:\n",
        "    valid_length = valid_length.repeat_interleave(n, dim=0)\n",
        "  else:\n",
        "    valid_length = valid_length.reshape((-1,))\n",
        "\n",
        "  mask = torch.arange(m)[None, :].to(X.device) >= valid_length[:, None]\n",
        "  X.view(-1, m)[mask] = mask_value\n",
        "\n",
        "  Y = torch.softmax(X, dim=-1)\n",
        "\n",
        "\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AYLKdWCXFPKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60cc113-38d5-4ad6-c1bb-39121d0857a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4667, 0.5333, 0.0000, 0.0000],\n",
              "         [0.5474, 0.4526, 0.0000, 0.0000]],\n",
              "\n",
              "        [[0.2324, 0.5569, 0.2107, 0.0000],\n",
              "         [0.3379, 0.4132, 0.2489, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WORGFF5FPKF"
      },
      "source": [
        "- **Scaled Dot Product Attention**\n",
        "    - The scaled dot-product attention uses the score function as: $\\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\mathbf{k}^T / \\sqrt{d}$, where $d$ is the dimension of query (which in this case is equal to the dimension of the keys). The following figures visualizes this process in matrix form, in which $Q \\in \\mathcal{R}^{m\\times d_k}, \\mathbf{K} \\in \\mathcal{R}^{n \\times d_k}$, and $\\mathbf{V} \\in \\mathcal{R}^{n \\times d_v}$.\n",
        "\n",
        "    <div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\n",
        "\n",
        "Implement the DotProductAttention below. Do not use any loops in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "id": "JfOAos0nFPKF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8b7a6c1703c6c230a006a6b86326a3b9",
          "grade": false,
          "grade_id": "cell-eac4fccbcd4f068e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(DotProductAttention, self).__init__()\n",
        "\n",
        "  def forward(self, query, key, value, valid_length=None):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of DotProductAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "    if valid_length is not None:\n",
        "        scores = masked_softmax(scores, valid_length)\n",
        "    else:\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    attention = torch.matmul(scores, value)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIZLE7C8FPKF"
      },
      "source": [
        "### Correctness Check for DotProductAttention\n",
        "\n",
        "Run the following snippet to check your implementation of DotProductAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000, 4.0000, 5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Wv3mrVJ0FPKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966669bd-3437-4635-ca79-2a3621db1ed2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "att = DotProductAttention()\n",
        "keys = torch.ones((2,10,2),dtype=torch.float)\n",
        "values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\n",
        "att(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DT_pDi5FPKG"
      },
      "source": [
        "- **MLP Attention**\n",
        "\n",
        "    In MLP attention, we project both query and keys into $R^h$, add the results, and use a $\\text{tanh}$ before multiplying by the values. The score function is defined as:\n",
        "    \n",
        "    $$\n",
        "    \\alpha(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\text{tanh}(W_k\\mathbf{k} + W_q\\mathbf{q})\n",
        "    $$\n",
        "    \n",
        "    where $\\mathbf{v}, \\mathbf{W_k}\\text{, and }\\mathbf{W_q}$ are learnable parameters.\n",
        "    \n",
        "Implement the MLP attention in matrix form without using any loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "id": "2ur7xGGHFPKH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9440f3519ad5f8037f192758aecca64a",
          "grade": false,
          "grade_id": "cell-6be727894d4fd817",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class MLPAttention(nn.Module):\n",
        "  def __init__(self, d_v, d_k, d_q):\n",
        "    super(MLPAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_k: feature dimension of key\n",
        "      d_v: feature dimension of vector v\n",
        "      d_q: feature dimension of query\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize learnable parameters\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.W_k = nn.Linear(d_k, d_v, bias=False)\n",
        "    self.W_q = nn.Linear(d_q, d_v, bias=False)\n",
        "    self.v = nn.Parameter(torch.rand(d_v, 1))\n",
        "    self.d_v = d_v\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, n, d)\n",
        "      key: tensor of size (B, m, d)\n",
        "      value: tensor of size (B, m, dim_v)\n",
        "      valid_length: either (B, )\n",
        "\n",
        "      B is the batch_size, n is the number of queries, m is the number of <key, value> pairs,\n",
        "      d is the feature dimension of the query, and dim_v is the feature dimension of the value.\n",
        "\n",
        "    Outputs:\n",
        "      attention: tensor of size (B, n, dim_v), weighted sum of values\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of MLPAttention. Do not\n",
        "    # use any loops in your implementation.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    query_proj = self.W_q(query)\n",
        "    key_proj = self.W_k(key)\n",
        "    query_proj_expanded = query_proj.unsqueeze(2)\n",
        "    key_proj_expanded = key_proj.unsqueeze(1)\n",
        "\n",
        "    features = torch.tanh(query_proj_expanded + key_proj_expanded)\n",
        "    scores = torch.matmul(features, self.v).squeeze(-1)  # (B, n, m)\n",
        "\n",
        "    if valid_length is not None:\n",
        "        mask_value = -1e7\n",
        "        mask = torch.arange(key.size(1))[None, :].to(query.device) >= valid_length[:, None]\n",
        "        scores.masked_fill_(mask, mask_value)\n",
        "\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    Y = torch.bmm(attn_weights, value)\n",
        "    # END OF YOUR CODE\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt2cl_zgFPKH"
      },
      "source": [
        "### Correctness Check for MLPAttention\n",
        "\n",
        "Run the following snippet to check your implementation of MLPAttention.\n",
        "\n",
        "Expected output:\n",
        "\n",
        "```\n",
        "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000],\n",
        "         [ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
        "\n",
        "        [[10.0000, 11.0000, 12.0000, 13.0000],\n",
        "         [10.0000, 11.0000, 12.0000, 13.0000]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZfT1SjdkFPKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f01872-0713-4690-95f7-a0440fa6b034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000],\n",
              "         [10.0000, 11.0000, 12.0000, 13.0000]],\n",
              "\n",
              "        [[ 2.0000,  3.0000,  4.0000,  5.0000],\n",
              "         [10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "atten = MLPAttention(4, 2, 2)\n",
        "atten(torch.ones((2,2,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmJHddOeFPKI"
      },
      "source": [
        "    \n",
        "- **Using Attention in seq2seq Models**\n",
        "\n",
        "    <div>\n",
        "    <img src=\"https://d2l.ai/_images/seq2seq-attention.svg\" width=\"600\"/>\n",
        "    </div>\n",
        "Image source: https://d2l.ai/_images/seq2seq-attention.svg\n",
        "\n",
        "    Now we want to add attention to the seq2seq model. As we previously stated, attention allows the decoder to have more direct access to previous states in the encoder. In the context of machine translation, when the decoder is predicting a word in the translation, it can focus on certain words in the original language. Therefore, we want the keys and the values of the attention layer to be the output of the encoder at each step. The query for the attention layer would be the decoder's previous hidden state. The output of the attention layer, referred to as the context, is concatenated with the decoder input and fed into the decoder.\n",
        "    \n",
        "    In rough pseudocode, this looks like:\n",
        "    ```\n",
        "    context = attention(query=h_prev, keys=encoder_output, values=encoder_output)\n",
        "    decoder_input = concatenate([decoder_input, context])\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl9vXv7EFPKI"
      },
      "source": [
        "### LSTM Encoder-Decoder\n",
        "\n",
        "\n",
        "Build a seq2seq model with LSTM and attention.\n",
        "\n",
        "- Complete the Encoder forward() function.\n",
        "- Complete the Decoder forward() and predict() functions. The decoder should utilize the attention mechanism.\n",
        "- Find a good learning rate for training this model. Feel free to add code here to test out different learning rates, but make sure that your best model is saved in `lstm_net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "deletable": false,
        "id": "pGBJB_klFPKJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbd2ff0f838eab4eeb305bd07bbd3a8e",
          "grade": false,
          "grade_id": "cell-85d8bda82bc92dd8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Encoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim, hidden_size, device)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, sources, valid_len):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder forward pass\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    batch_size, seq_len = sources.shape\n",
        "    embedd = self.embedding(sources)\n",
        "    c = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "    h = torch.zeros((batch_size, self.hidden_size)).to(device)\n",
        "    outputs, (h, c) = self.enc((embedd, valid_len), (h, c))\n",
        "    # END OF YOUR CODE\n",
        "    return outputs, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "id": "QruLtgx5FPKJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8f15ad91df74611a4f70744a202ad53",
          "grade": false,
          "grade_id": "cell-154ce877082ed913",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(Decoder, self).__init__()\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      vocab_size: int, the number of words in the vocabulary\n",
        "      embedding_dim: int, dimension of the word embedding\n",
        "      hidden_size: int, dimension of vallina RNN\n",
        "    \"\"\"\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.enc = LSTM(embedding_dim+hidden_size, hidden_size, device)\n",
        "    self.att = DotProductAttention()\n",
        "    self.output_emb = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, state, target, valid_len):\n",
        "    loss = 0\n",
        "    preds = []\n",
        "\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Decoder forward pass. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    target_embeddings = self.embedding(target)\n",
        "    batch_size, seq_len = target_embeddings.shape[:2]\n",
        "    o, (h, c), src_len = state\n",
        "\n",
        "    for i in range(0, seq_len - 1):\n",
        "      decoder_input = self.embedding(target[:, i]).unsqueeze(1)\n",
        "      attention_weights = self.att(h.unsqueeze(1), o, o, src_len)\n",
        "      decoder_input = torch.cat([decoder_input, attention_weights], dim = -1)\n",
        "      o, (h, c) = self.enc((decoder_input, src_len), (h, c))\n",
        "      out = self.output_emb(o)\n",
        "      preds.append(out)\n",
        "      loss += F.nll_loss(F.log_softmax(out[:, 0]), target[:, i + 1])\n",
        "    # END OF YOUR CODE\n",
        "    return loss, preds\n",
        "\n",
        "  def predict(self, state, target, valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement LSTM Encoder prediction. Your solution should also use\n",
        "    # self.att for attention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    predicts = []\n",
        "    inp = self.embedding(target[:, :1])\n",
        "    o, (h, c), size_src = state\n",
        "\n",
        "    for i in range(0, MAX_LEN + 1):\n",
        "      attention_weights = self.att(h.unsqueeze(1), o, o, size_src)\n",
        "      inp = torch.cat([inp, attention_weights], dim = -1)\n",
        "      o, (h,c) = self.enc((inp, size_src), (h, c))\n",
        "      pred = self.output_emb(o)\n",
        "      predicts.append(pred)\n",
        "      inp = self.embedding(pred.argmax(dim = -1))\n",
        "\n",
        "    pred = torch.cat(predicts, dim = 1).argmax(dim = -1)\n",
        "    # END OF YOUR CODE\n",
        "    return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PHD8WWgiFPKJ"
      },
      "outputs": [],
      "source": [
        "class NMTLSTM(nn.Module):\n",
        "  def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, hidden_size, device):\n",
        "    super(NMTLSTM, self).__init__()\n",
        "    self.enc = Encoder(src_vocab_size, embedding_dim, hidden_size, device)\n",
        "    self.dec = Decoder(tgt_vocab_size, embedding_dim, hidden_size, device)\n",
        "\n",
        "  def forward(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    loss, pred = self.dec((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return loss, pred\n",
        "\n",
        "  def predict(self, src, src_len, tgt, tgt_len):\n",
        "    outputs, (h, c) = self.enc(src, src_len)\n",
        "    pred = self.dec.predict((outputs, (h, c), src_len), tgt, tgt_len)\n",
        "    return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "id": "_935PwXWFPKK",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "73bc6101f369326d21d0efe8439c652b",
          "grade": false,
          "grade_id": "cell-bfaaa623c7199b2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11db5e40-1db8-4e9a-832f-d4a79817d918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-457460d3a84c>:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss += F.nll_loss(F.log_softmax(out[:, 0]), target[:, i + 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 / 7800\tLoss:\t54.438988\n",
            "pred:\t tensor([[[-0.1352, -0.1829, -0.1683,  ..., -0.0786, -0.0933, -0.0429]],\n",
            "\n",
            "        [[-0.0862, -0.1818, -0.1682,  ..., -0.0852, -0.1434, -0.0266]],\n",
            "\n",
            "        [[-0.0873, -0.2143, -0.1827,  ..., -0.0818, -0.1497, -0.0362]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1004, -0.1912, -0.1695,  ..., -0.0996, -0.1242, -0.0527]],\n",
            "\n",
            "        [[-0.0916, -0.2093, -0.1868,  ..., -0.1000, -0.1424, -0.0278]],\n",
            "\n",
            "        [[-0.0922, -0.1787, -0.1895,  ..., -0.1014, -0.1149, -0.0470]]])\n",
            "\n",
            "tgt:\t tensor([ 38, 338,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t13.556274\n",
            "pred:\t tensor([[[-2.6428e-01, -2.8474e+00, -5.6578e-02,  ..., -1.7079e+00,\n",
            "          -1.3442e+00, -1.9930e+00]],\n",
            "\n",
            "        [[-9.2018e-02, -2.9212e+00,  2.7628e-03,  ..., -1.6792e+00,\n",
            "          -1.1604e+00, -1.9569e+00]],\n",
            "\n",
            "        [[-3.0523e-01, -3.0912e+00, -2.6962e-01,  ..., -1.7787e+00,\n",
            "          -1.4632e+00, -2.0799e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 3.2608e-01, -3.0046e+00,  1.7078e-01,  ..., -1.7623e+00,\n",
            "          -1.5079e+00, -2.0682e+00]],\n",
            "\n",
            "        [[ 3.0293e-02, -3.1176e+00, -3.4970e-01,  ..., -1.9515e+00,\n",
            "          -1.6906e+00, -2.2491e+00]],\n",
            "\n",
            "        [[ 5.1036e-01, -3.0785e+00,  7.7864e-03,  ..., -1.8516e+00,\n",
            "          -1.6120e+00, -2.0107e+00]]])\n",
            "\n",
            "tgt:\t tensor([ 97,   9,  74, 172,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t11.528358\n",
            "pred:\t tensor([[[-0.2653, -3.8557, -1.3049,  ..., -1.5259, -1.1554, -2.2506]],\n",
            "\n",
            "        [[ 0.7408, -3.3711,  0.3353,  ..., -1.7898, -1.4453, -1.8965]],\n",
            "\n",
            "        [[ 0.2015, -3.2828, -0.5371,  ..., -1.4360, -0.8868, -2.1170]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1116, -4.1320, -1.7054,  ..., -1.6856, -1.3818, -2.3103]],\n",
            "\n",
            "        [[ 1.0141, -3.7598, -1.1093,  ..., -1.5184, -1.6126, -2.0872]],\n",
            "\n",
            "        [[-0.1741, -3.3585, -1.1255,  ..., -1.2097, -0.6562, -2.0243]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t8.371086\n",
            "pred:\t tensor([[[ 1.0003, -4.1351, -1.9404,  ..., -1.6029, -1.4566, -2.0960]],\n",
            "\n",
            "        [[ 0.6963, -4.3431, -1.5207,  ..., -1.5490, -1.7154, -2.5675]],\n",
            "\n",
            "        [[ 0.2110, -3.9414, -2.2754,  ..., -1.2708, -1.1924, -2.4277]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1004, -3.8627, -0.5034,  ..., -1.9595, -1.7533, -2.5503]],\n",
            "\n",
            "        [[ 0.2976, -4.1187, -2.8418,  ..., -1.3199, -1.1478, -2.2011]],\n",
            "\n",
            "        [[ 0.9252, -3.5594, -1.4279,  ..., -1.4392, -1.2774, -1.9717]]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 134,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t6.969880\n",
            "pred:\t tensor([[[ 0.7688, -4.6104, -2.1759,  ..., -1.5628, -1.6339, -2.6321]],\n",
            "\n",
            "        [[-0.7276, -4.7232, -2.0298,  ..., -1.4020, -0.2888, -2.2647]],\n",
            "\n",
            "        [[ 0.3417, -3.8922, -1.4271,  ..., -1.3460, -0.7487, -1.6662]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.5916, -5.1533, -2.5191,  ..., -2.2126, -1.1951, -2.2045]],\n",
            "\n",
            "        [[-0.5823, -4.9658, -0.8464,  ..., -1.5523, -0.5254, -2.5294]],\n",
            "\n",
            "        [[ 0.7350, -4.1727, -1.7818,  ..., -1.2294, -1.0540, -2.1831]]])\n",
            "\n",
            "tgt:\t tensor([ 14, 385, 230,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t6.246637\n",
            "pred:\t tensor([[[-0.9218, -4.1164, -1.3439,  ..., -1.6401, -1.2102, -0.9535]],\n",
            "\n",
            "        [[ 0.6068, -4.3840, -1.9132,  ..., -0.7152, -1.5025, -2.2265]],\n",
            "\n",
            "        [[ 0.0106, -3.9084, -1.2428,  ..., -0.8111, -0.6248, -1.2526]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.9750, -5.1741, -0.5643,  ..., -1.5342, -1.9213, -2.4277]],\n",
            "\n",
            "        [[-2.0726, -4.5041, -0.4045,  ..., -1.3954,  0.5573, -1.4622]],\n",
            "\n",
            "        [[ 0.7094, -4.7828, -0.7429,  ..., -1.4292, -1.5853, -2.4738]]])\n",
            "\n",
            "tgt:\t tensor([ 38, 370,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t6.665478\n",
            "pred:\t tensor([[[ 0.3973, -3.9786, -1.9521,  ..., -0.5500, -1.1194, -2.1464]],\n",
            "\n",
            "        [[-0.3272, -5.7465, -0.6459,  ..., -1.2428, -0.8974, -3.1846]],\n",
            "\n",
            "        [[-0.6896, -4.4675, -0.7428,  ..., -0.3406,  0.2353, -2.5208]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.9411, -3.7221, -0.4497,  ..., -0.7465, -1.6922, -2.0496]],\n",
            "\n",
            "        [[ 1.0303, -4.4722, -2.2000,  ..., -0.8117, -0.9510, -1.7443]],\n",
            "\n",
            "        [[ 0.3973, -3.9786, -1.9521,  ..., -0.5500, -1.1194, -2.1464]]])\n",
            "\n",
            "tgt:\t tensor([290, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t5.471929\n",
            "pred:\t tensor([[[-1.9439e+00, -5.3986e+00, -3.2573e+00,  ..., -5.8406e-02,\n",
            "           2.0040e-01, -2.2252e+00]],\n",
            "\n",
            "        [[-1.7542e+00, -4.4086e+00, -2.3492e+00,  ..., -1.4386e+00,\n",
            "           5.5365e-03, -2.1810e+00]],\n",
            "\n",
            "        [[ 4.6845e-01, -5.5907e+00, -2.7324e+00,  ..., -6.4044e-02,\n",
            "          -2.0935e+00, -3.1592e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-9.8254e-01, -5.0193e+00, -1.9184e+00,  ..., -2.0301e+00,\n",
            "          -1.8723e+00, -1.0533e+00]],\n",
            "\n",
            "        [[-1.0146e-02, -4.1070e+00, -2.3863e+00,  ..., -1.9316e-01,\n",
            "          -1.1713e+00, -1.9221e+00]],\n",
            "\n",
            "        [[-7.6993e-01, -4.0598e+00, -1.1887e+00,  ...,  2.3074e-01,\n",
            "          -8.6829e-02, -9.7648e-01]]])\n",
            "\n",
            "tgt:\t tensor([  3,  74, 129,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t5.326020\n",
            "pred:\t tensor([[[-0.5391, -4.5202, -1.0949,  ..., -0.7728, -0.2469, -1.4226]],\n",
            "\n",
            "        [[-1.5024, -5.3166, -2.9089,  ..., -0.9200,  0.0334, -2.6827]],\n",
            "\n",
            "        [[ 0.3945, -4.3857, -0.8681,  ..., -0.2995, -1.4498, -1.7525]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.7799, -5.7529, -0.3544,  ..., -1.3293, -2.1285, -2.9729]],\n",
            "\n",
            "        [[-0.9718, -4.9481, -2.3028,  ..., -1.7058, -0.7886, -2.3798]],\n",
            "\n",
            "        [[ 1.0031, -5.3843, -1.5387,  ..., -0.8582, -1.5853, -2.1531]]])\n",
            "\n",
            "tgt:\t tensor([ 15, 204,  37,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t3.755133\n",
            "pred:\t tensor([[[-1.8332, -4.8268, -2.8078,  ..., -0.7106,  2.1927, -2.2289]],\n",
            "\n",
            "        [[-0.5062, -5.0082, -3.5542,  ..., -0.2191, -0.1267, -2.7251]],\n",
            "\n",
            "        [[-2.3786, -4.3621, -2.8980,  ..., -0.7507,  1.2525, -2.0731]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.9445, -6.3575, -0.9604,  ..., -1.2752, -2.2886, -2.9597]],\n",
            "\n",
            "        [[-0.7560, -4.2050, -2.1308,  ..., -1.2109, -0.9630, -0.7124]],\n",
            "\n",
            "        [[-2.4411, -4.7531, -1.9816,  ..., -1.7395,  1.2102, -1.0030]]])\n",
            "\n",
            "tgt:\t tensor([154,  37, 413,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t4.391334\n",
            "pred:\t tensor([[[-1.0534, -3.8885, -1.9221,  ...,  1.0078,  1.1345, -1.9994]],\n",
            "\n",
            "        [[ 0.1369, -4.2313, -2.2895,  ...,  0.1924, -1.3888, -0.9352]],\n",
            "\n",
            "        [[-0.6633, -5.7205, -2.4977,  ..., -1.7974, -1.0602, -1.7617]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0518, -3.8128, -1.4010,  ..., -0.0967, -2.0579, -2.1318]],\n",
            "\n",
            "        [[-1.9509, -6.2269, -2.4087,  ..., -0.7391,  0.5584, -3.4783]],\n",
            "\n",
            "        [[-1.7430, -5.1440, -0.9177,  ..., -2.0975,  1.1030, -1.5052]]])\n",
            "\n",
            "tgt:\t tensor([321,  75,  45, 322,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t3.815242\n",
            "pred:\t tensor([[[-1.2675, -5.6499, -2.4649,  ..., -2.0620, -2.1338, -0.9492]],\n",
            "\n",
            "        [[-1.6713, -4.6067, -3.9507,  ...,  0.7001,  0.7775, -2.1258]],\n",
            "\n",
            "        [[ 0.5683, -4.7760, -0.9398,  ..., -0.5747, -1.7400, -1.9965]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.8987, -6.2915, -2.6304,  ..., -1.8370, -0.9829, -2.0561]],\n",
            "\n",
            "        [[ 0.9102, -5.2018, -1.0042,  ..., -0.3776, -1.8926, -1.8141]],\n",
            "\n",
            "        [[ 1.3051, -6.2535,  0.0298,  ..., -0.7053, -2.5078, -3.3601]]])\n",
            "\n",
            "tgt:\t tensor([38, 91,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t3.400280\n",
            "pred:\t tensor([[[-0.1806, -5.6586,  0.8596,  ..., -0.9220,  0.2836, -3.4997]],\n",
            "\n",
            "        [[-0.0803, -4.6246, -2.3341,  ..., -1.1812,  0.0655, -1.3012]],\n",
            "\n",
            "        [[-1.2343, -7.4382, -2.7115,  ..., -1.9222, -1.9724, -3.5487]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.2213, -5.3793, -1.8991,  ...,  0.7008,  0.8641, -2.9136]],\n",
            "\n",
            "        [[-0.0312, -4.6404, -0.1554,  ..., -0.7397, -1.2109, -2.9513]],\n",
            "\n",
            "        [[-1.6034, -4.2554, -2.8333,  ..., -0.6353,  1.0971, -3.2753]]])\n",
            "\n",
            "tgt:\t tensor([266,   3,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t3.877556\n",
            "pred:\t tensor([[[-2.2116, -4.9266, -2.7531,  ..., -2.2601,  0.2320, -3.2018]],\n",
            "\n",
            "        [[-1.2788, -6.5255, -2.9977,  ...,  1.8001, -2.6187, -3.1569]],\n",
            "\n",
            "        [[-2.6173, -6.9429, -3.6058,  ..., -1.8549, -0.8248, -2.8260]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.8317, -4.5761, -0.3520,  ..., -0.1695,  0.4138, -1.8762]],\n",
            "\n",
            "        [[ 0.9934, -3.8911, -0.0288,  ..., -0.4905, -1.8555, -0.5493]],\n",
            "\n",
            "        [[-0.1901, -4.6934, -2.8949,  ...,  0.4761,  0.1371, -2.1150]]])\n",
            "\n",
            "tgt:\t tensor([156, 157, 323,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t2.898418\n",
            "pred:\t tensor([[[-0.4362, -4.7940, -1.5103,  ..., -0.3424, -1.8157,  0.3236]],\n",
            "\n",
            "        [[-2.6361, -5.3706, -2.8714,  ..., -1.3427,  0.4233, -1.7596]],\n",
            "\n",
            "        [[-1.8786, -6.5976, -2.7317,  ...,  0.0679, -0.7411, -3.7149]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1790, -4.1879,  0.2060,  ...,  1.4495, -1.0971, -2.1076]],\n",
            "\n",
            "        [[-0.8415, -6.1557, -3.0149,  ...,  2.7131, -2.3209, -3.2440]],\n",
            "\n",
            "        [[-0.5712, -4.3957, -1.1794,  ..., -0.4714,  3.0864, -2.2418]]])\n",
            "\n",
            "tgt:\t tensor([ 38, 116, 202, 112, 148,  11,   2,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t3.109351\n",
            "pred:\t tensor([[[ 1.2010, -6.9293, -1.7134,  ..., -0.7054, -2.8724, -2.7801]],\n",
            "\n",
            "        [[-1.6464, -6.4628, -1.7440,  ..., -2.2289, -3.0996, -0.9180]],\n",
            "\n",
            "        [[ 0.3699, -5.9099, -1.5981,  ..., -0.6376, -0.9991, -2.7618]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1019, -4.7639, -2.4648,  ...,  0.6000, -0.6218, -0.9460]],\n",
            "\n",
            "        [[ 0.0613, -5.0066, -2.4192,  ...,  0.7051,  0.0695, -3.0893]],\n",
            "\n",
            "        [[-2.5006, -5.5850, -3.3429,  ..., -0.2031,  2.0046, -4.1354]]])\n",
            "\n",
            "tgt:\t tensor([ 14,   3, 177,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t2.613095\n",
            "pred:\t tensor([[[ 0.3567, -4.7578, -0.8093,  ..., -0.6084, -3.2512, -3.2641]],\n",
            "\n",
            "        [[-1.4546, -6.6360, -2.8355,  ..., -1.5875, -0.1140, -2.8733]],\n",
            "\n",
            "        [[-2.5501, -4.4708, -2.8898,  ..., -0.8802,  1.8628, -2.5285]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0859, -6.1387, -2.6632,  ...,  0.2603, -3.0350, -2.8391]],\n",
            "\n",
            "        [[ 0.0158, -4.7038, -1.4026,  ...,  0.4128, -1.4065, -2.0813]],\n",
            "\n",
            "        [[ 0.8307, -6.7936, -1.0275,  ..., -0.7225, -3.0777, -3.6159]]])\n",
            "\n",
            "tgt:\t tensor([ 14,  79,  28,  41, 221,  11,   2,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t2.242239\n",
            "pred:\t tensor([[[-0.8056, -4.7139, -3.5019,  ...,  0.8319,  0.5807, -2.8729]],\n",
            "\n",
            "        [[-0.9226, -6.0498, -2.7658,  ..., -0.4740,  1.9722, -3.1974]],\n",
            "\n",
            "        [[ 0.0106, -5.9145, -0.9406,  ..., -1.4398,  0.0228, -3.5430]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.2502, -6.1176, -2.6928,  ...,  0.3787,  0.6966, -3.5817]],\n",
            "\n",
            "        [[-2.1031, -5.4876, -3.8230,  ..., -0.8477,  1.0512, -3.4310]],\n",
            "\n",
            "        [[-1.5419, -4.8949, -3.6820,  ...,  0.8064, -0.5574, -3.5223]]])\n",
            "\n",
            "tgt:\t tensor([55, 56,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t2.157280\n",
            "pred:\t tensor([[[-2.2046, -4.5933, -1.7509,  ...,  0.1089,  0.9262, -1.9621]],\n",
            "\n",
            "        [[-1.0060, -7.0616, -1.6255,  ...,  0.1877,  0.5947, -3.9271]],\n",
            "\n",
            "        [[-0.0655, -6.6994, -3.9090,  ...,  0.0483, -2.6875, -3.8838]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0290, -5.8430, -2.1283,  ..., -0.0910, -1.4994, -2.7804]],\n",
            "\n",
            "        [[-2.3577, -6.0700, -1.2734,  ..., -1.4776,  1.4792, -4.2184]],\n",
            "\n",
            "        [[-0.8169, -4.9567, -3.7219,  ...,  1.1035, -0.0829, -1.8643]]])\n",
            "\n",
            "tgt:\t tensor([168,  90,  79,   4,  41,  24,   2,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t2.777786\n",
            "pred:\t tensor([[[-2.4961, -5.9796, -5.6878,  ..., -0.4453, -2.2897, -4.7479]],\n",
            "\n",
            "        [[-1.4626, -5.5409, -1.7701,  ...,  1.7274,  0.4618, -3.2937]],\n",
            "\n",
            "        [[-2.6932, -7.6954, -3.5800,  ..., -2.3483, -2.1933, -3.9953]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.6392, -5.6382, -2.2674,  ...,  0.1210, -2.4846, -1.3237]],\n",
            "\n",
            "        [[-0.1359, -6.7223, -3.1627,  ..., -0.3414, -4.6595, -4.4356]],\n",
            "\n",
            "        [[ 0.5537, -7.6797, -1.2647,  ..., -0.2208, -3.8279, -4.1256]]])\n",
            "\n",
            "tgt:\t tensor([303, 314,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t1.814118\n",
            "pred:\t tensor([[[-2.5821, -7.5159, -3.8702,  ..., -1.9427, -1.9472, -4.0377]],\n",
            "\n",
            "        [[-1.0090, -6.9913, -4.1596,  ..., -1.1393, -2.2426, -3.4775]],\n",
            "\n",
            "        [[-2.6532, -6.9242, -3.7925,  ..., -1.1920,  0.2170, -3.8000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4339, -6.7080, -4.5897,  ..., -1.1617, -1.2846, -3.9484]],\n",
            "\n",
            "        [[-0.8644, -6.3223, -4.4894,  ..., -2.0839, -1.4371, -2.0896]],\n",
            "\n",
            "        [[-2.3238, -5.2459, -2.8108,  ..., -0.5184,  1.8480, -2.5425]]])\n",
            "\n",
            "tgt:\t tensor([52, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t2.278133\n",
            "pred:\t tensor([[[-1.5561, -4.8882, -1.2585,  ...,  0.1009,  2.0109, -2.0851]],\n",
            "\n",
            "        [[-0.6575, -4.2081, -1.8427,  ...,  0.9548, -2.4936, -0.7596]],\n",
            "\n",
            "        [[-0.7255, -7.1715, -3.2396,  ...,  0.0351,  0.1467, -3.2718]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4074, -7.8457, -2.1876,  ..., -0.8173,  1.0541, -3.4247]],\n",
            "\n",
            "        [[ 0.6843, -7.8534, -1.0521,  ..., -0.6985, -3.6706, -4.5302]],\n",
            "\n",
            "        [[-0.6689, -7.0715, -1.1416,  ..., -0.4656, -0.1955, -4.0688]]])\n",
            "\n",
            "tgt:\t tensor([  9, 167,  45,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t2.698822\n",
            "pred:\t tensor([[[-2.0168, -5.8723, -4.5208,  ...,  2.1578, -0.3092, -3.0544]],\n",
            "\n",
            "        [[-1.7750, -4.2628, -2.6408,  ..., -1.8779,  3.8951, -3.1035]],\n",
            "\n",
            "        [[-0.4035, -6.7517, -2.6669,  ...,  0.7897, -2.7697, -3.7464]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.4636, -7.6512, -1.4790,  ..., -0.4999, -3.7953, -3.9916]],\n",
            "\n",
            "        [[-0.2790, -6.8709, -3.7621,  ..., -1.0926, -3.0304, -4.1098]],\n",
            "\n",
            "        [[-2.6281, -6.6144, -3.3426,  ..., -0.9217,  0.3208, -3.7778]]])\n",
            "\n",
            "tgt:\t tensor([  3, 320,  48,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t2.118217\n",
            "pred:\t tensor([[[ 0.0969, -6.4305, -2.9484,  ...,  1.1548, -2.7986, -3.5899]],\n",
            "\n",
            "        [[ 0.5546, -6.1159, -2.9189,  ...,  1.2899, -3.0357, -3.2117]],\n",
            "\n",
            "        [[-0.3142, -4.2062, -1.4846,  ...,  1.3025, -4.0261, -1.9735]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1299, -4.9434, -1.3780,  ...,  0.5431, -3.3265, -2.1154]],\n",
            "\n",
            "        [[-0.2211, -4.8565, -0.9526,  ...,  2.5315, -4.3708, -3.1046]],\n",
            "\n",
            "        [[-2.5743, -6.4311, -1.4504,  ...,  0.6278, -0.0928, -4.6654]]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116,  72,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t1.654847\n",
            "pred:\t tensor([[[ 0.4819, -6.1006, -1.1281,  ..., -1.3585, -4.0197, -4.0308]],\n",
            "\n",
            "        [[-1.4811, -6.2429, -1.1935,  ...,  1.3908, -1.8593, -2.8128]],\n",
            "\n",
            "        [[ 0.9310, -7.2348, -3.7693,  ..., -0.1504, -4.9294, -5.3123]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.4044, -4.2355, -2.5751,  ..., -0.9496,  3.8546, -3.4830]],\n",
            "\n",
            "        [[-3.9916, -5.7268, -3.3233,  ...,  0.7657, -0.3794, -1.8274]],\n",
            "\n",
            "        [[ 0.6217, -7.5248, -0.7742,  ..., -1.6025, -4.5729, -5.7130]]])\n",
            "\n",
            "tgt:\t tensor([14, 28, 54, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t1.580140\n",
            "pred:\t tensor([[[-2.9217, -6.8112, -2.8075,  ..., -1.6850,  2.4443, -4.5535]],\n",
            "\n",
            "        [[-2.1603, -8.1339, -4.1460,  ..., -2.5518, -2.4890, -5.3782]],\n",
            "\n",
            "        [[-2.7426, -4.6740, -0.8559,  ..., -0.9411,  1.8823, -1.2014]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.3051, -7.4355, -4.0068,  ..., -2.2607, -2.3700, -4.3228]],\n",
            "\n",
            "        [[-2.0070, -6.0644, -2.6091,  ...,  1.2389, -0.5408, -3.1387]],\n",
            "\n",
            "        [[-2.7449, -5.7501, -5.4387,  ...,  1.4155, -0.0789, -1.8926]]])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t1.465870\n",
            "pred:\t tensor([[[ 0.5173, -8.1921, -1.4399,  ..., -0.9672, -4.7554, -5.1000]],\n",
            "\n",
            "        [[-2.6804, -7.7878, -1.8994,  ..., -3.4566,  1.0958, -3.2061]],\n",
            "\n",
            "        [[ 0.5357, -5.8827, -2.4138,  ...,  1.0455, -2.6230, -1.7786]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.1512, -5.0707, -4.5983,  ..., -0.3005,  0.9243, -2.5166]],\n",
            "\n",
            "        [[-2.0106, -6.4293, -1.8978,  ..., -0.3982, -0.0438, -2.5295]],\n",
            "\n",
            "        [[-2.4756, -4.8524, -6.3238,  ...,  0.7720,  0.0685, -2.0843]]])\n",
            "\n",
            "tgt:\t tensor([14, 28,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t1.555629\n",
            "pred:\t tensor([[[-0.2554, -7.1638, -3.0588,  ..., -0.9081, -4.6609, -4.8938]],\n",
            "\n",
            "        [[-2.1831, -6.4069, -4.0822,  ..., -3.0935, -3.6550, -0.6013]],\n",
            "\n",
            "        [[-2.0278, -6.4546, -6.0062,  ...,  0.0211, -2.5470, -3.0872]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.6761, -7.9440, -4.7275,  ..., -1.2794, -4.6237, -3.2601]],\n",
            "\n",
            "        [[-1.6932, -5.5708, -1.6232,  ...,  0.7832,  2.0523, -2.4097]],\n",
            "\n",
            "        [[-0.2294, -6.4140,  1.6066,  ..., -2.0551, -0.8398, -4.3129]]])\n",
            "\n",
            "tgt:\t tensor([14,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t1.833169\n",
            "pred:\t tensor([[[-2.9413, -7.5178, -4.6352,  ..., -2.5401, -3.4386, -1.7284]],\n",
            "\n",
            "        [[-0.0489, -5.3095, -3.2849,  ..., -1.0991, -1.0022,  1.4472]],\n",
            "\n",
            "        [[-3.0400, -5.2948, -2.3844,  ...,  1.3684,  2.1888, -3.0602]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0308, -5.1417, -2.7243,  ...,  0.6426, -5.2039, -2.0372]],\n",
            "\n",
            "        [[-1.6928, -5.1105, -4.5198,  ..., -2.8980, -0.1130, -1.1549]],\n",
            "\n",
            "        [[-2.4807, -5.0866, -3.7005,  ..., -2.4337,  2.9375, -2.7575]]])\n",
            "\n",
            "tgt:\t tensor([38,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t1.364934\n",
            "pred:\t tensor([[[ 0.9062, -5.6797, -1.7066,  ...,  1.7199, -3.8918, -1.9086]],\n",
            "\n",
            "        [[-2.0333, -6.1851, -1.5659,  ..., -0.8829,  0.8446, -1.7890]],\n",
            "\n",
            "        [[-3.0641, -5.4973, -3.2612,  ..., -0.0814,  1.4989, -3.1372]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.7899, -6.4256, -1.5693,  ..., -0.7721, -1.9964, -3.4241]],\n",
            "\n",
            "        [[-1.4014, -6.2362, -3.1591,  ..., -0.5982,  0.7487, -2.8414]],\n",
            "\n",
            "        [[-1.9521, -5.7378, -3.7907,  ...,  0.6695, -0.8951, -2.1155]]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179,   3, 259,  11,   2,   0,   0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t1.342125\n",
            "pred:\t tensor([[[ 0.7810, -6.8902, -1.9793,  ..., -2.0483, -3.0341, -1.8407]],\n",
            "\n",
            "        [[ 0.8335, -5.5971, -2.8317,  ..., -1.0153, -2.3991, -1.6861]],\n",
            "\n",
            "        [[-2.1533, -7.2510, -4.3375,  ..., -2.2317,  0.2688, -4.9587]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0153, -5.6567, -3.1659,  ...,  0.2684, -3.2608, -2.9223]],\n",
            "\n",
            "        [[-1.9620, -6.8075, -5.7774,  ..., -1.1907, -2.5881, -3.6543]],\n",
            "\n",
            "        [[ 0.2465, -7.4964, -3.2291,  ...,  0.0926, -5.3446, -3.1369]]])\n",
            "\n",
            "tgt:\t tensor([15,  3, 75,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t1.423741\n",
            "pred:\t tensor([[[ 0.4774, -8.3401, -1.2667,  ..., -1.1536, -4.9877, -5.0444]],\n",
            "\n",
            "        [[-1.6971, -6.7646, -3.9213,  ..., -3.5241, -1.3469, -3.5003]],\n",
            "\n",
            "        [[ 1.2036, -4.4432, -0.5243,  ..., -1.3592, -4.1631, -0.7956]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.1330, -5.8622, -1.6658,  ..., -2.3061,  3.3289, -3.9996]],\n",
            "\n",
            "        [[-1.2922, -4.4331, -3.9657,  ...,  1.4379,  0.9725, -1.8979]],\n",
            "\n",
            "        [[-0.1634, -4.4921, -0.7645,  ..., -1.7778, -1.7421, -3.0909]]])\n",
            "\n",
            "tgt:\t tensor([ 15, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t1.331873\n",
            "pred:\t tensor([[[-0.4239, -5.0746, -2.3155,  ...,  1.2145, -3.0648, -2.1602]],\n",
            "\n",
            "        [[-3.6647, -5.7087, -3.0559,  ..., -4.2628, -1.7289, -0.5112]],\n",
            "\n",
            "        [[-1.5814, -5.5216, -3.3901,  ...,  0.5106,  1.8604, -2.5288]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.4627, -4.5027, -2.1038,  ..., -1.2581,  0.8695, -1.7749]],\n",
            "\n",
            "        [[-1.3663, -8.0188, -2.0598,  ..., -1.5107, -3.3174, -4.4277]],\n",
            "\n",
            "        [[-3.0088, -5.8504, -3.0910,  ..., -1.8498, -0.5270, -1.7547]]])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179, 259,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t1.377544\n",
            "pred:\t tensor([[[-2.0727e+00, -6.2202e+00, -4.8347e+00,  ..., -2.7847e+00,\n",
            "           1.5591e+00, -1.8994e+00]],\n",
            "\n",
            "        [[-2.4618e-01, -6.7355e+00,  2.0429e+00,  ..., -2.7533e+00,\n",
            "          -1.1551e+00, -4.7662e+00]],\n",
            "\n",
            "        [[ 1.5458e-03, -5.4345e+00, -1.5010e+00,  ..., -7.7669e-01,\n",
            "          -3.8899e+00, -4.3911e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.7759e+00, -6.3698e+00, -2.7282e+00,  ..., -4.4569e+00,\n",
            "          -5.1208e+00, -3.2240e+00]],\n",
            "\n",
            "        [[-2.9944e+00, -8.7373e+00, -4.9381e+00,  ..., -3.8386e+00,\n",
            "          -3.5445e+00, -4.9745e+00]],\n",
            "\n",
            "        [[-5.3761e-01, -8.2914e+00, -3.9732e+00,  ...,  9.8303e-01,\n",
            "          -6.3060e+00, -4.7553e+00]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t1.335779\n",
            "pred:\t tensor([[[-1.4439, -5.4305, -1.7428,  ..., -0.9700, -2.6975, -0.4292]],\n",
            "\n",
            "        [[-1.5793, -6.1931, -0.3558,  ..., -1.1444,  0.6535, -2.8729]],\n",
            "\n",
            "        [[-3.0838, -7.1901, -4.2810,  ..., -3.8468,  0.0785, -1.6887]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.2269, -5.7469, -4.5627,  ...,  0.6407, -2.5303, -3.9621]],\n",
            "\n",
            "        [[-0.7212, -4.1307, -1.8385,  ...,  1.0997, -2.4544, -4.7974]],\n",
            "\n",
            "        [[-1.1822, -5.6175, -4.0198,  ..., -0.5455, -1.9957, -4.9496]]])\n",
            "\n",
            "tgt:\t tensor([ 15, 204,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t1.441935\n",
            "pred:\t tensor([[[-4.4051, -6.0833, -2.9629,  ...,  1.4875, -1.6802, -1.6765]],\n",
            "\n",
            "        [[-0.8901, -4.1153, -2.4314,  ..., -1.3217, -3.5001, -4.0242]],\n",
            "\n",
            "        [[-1.9397, -6.1805, -3.2750,  ..., -1.1100,  0.2568, -2.4848]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0451, -6.7104, -4.3824,  ..., -0.7412, -5.3199, -4.8464]],\n",
            "\n",
            "        [[ 1.3299, -2.9331, -1.8116,  ..., -1.2885, -4.3006, -1.1452]],\n",
            "\n",
            "        [[-1.0392, -5.1828, -2.9505,  ..., -0.5070, -0.0616, -2.8286]]])\n",
            "\n",
            "tgt:\t tensor([51,  9,  3, 90,  5,  2,  0,  0,  0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t0.998862\n",
            "pred:\t tensor([[[-2.4667, -5.7591, -3.7022,  ..., -2.3426, -0.3297, -0.7648]],\n",
            "\n",
            "        [[-2.7199, -7.0813, -3.0672,  ..., -2.5244,  1.1619, -3.1838]],\n",
            "\n",
            "        [[-0.4374, -4.3499, -1.6011,  ...,  2.1252, -0.2424, -2.7089]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.1596, -4.9517, -2.5668,  ..., -1.0694,  2.1848, -2.4686]],\n",
            "\n",
            "        [[-0.3745, -5.4120, -1.5366,  ...,  0.4282, -5.5909, -2.8507]],\n",
            "\n",
            "        [[-2.5472, -8.1395, -5.4836,  ..., -3.1507, -5.0608, -4.8583]]])\n",
            "\n",
            "tgt:\t tensor([170,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t1.527054\n",
            "pred:\t tensor([[[-2.1523, -6.2134, -1.1252,  ...,  0.9820, -2.5317, -2.5981]],\n",
            "\n",
            "        [[ 0.3242, -8.5414, -1.5283,  ..., -1.7746, -5.6478, -5.3349]],\n",
            "\n",
            "        [[ 0.2024, -5.7853, -2.9542,  ..., -0.1252, -3.8718, -1.4732]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.4120, -8.0435, -4.0451,  ..., -2.3464, -0.8562, -5.6945]],\n",
            "\n",
            "        [[-3.5292, -8.0522, -6.0701,  ..., -2.2478, -1.4991, -3.7471]],\n",
            "\n",
            "        [[-2.1523, -6.2134, -1.1252,  ...,  0.9820, -2.5317, -2.5981]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 71, 81,  3, 24,  2,  0,  0,  0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t1.436859\n",
            "pred:\t tensor([[[-1.8088, -5.3587, -2.9562,  ..., -3.2443, -4.9838, -2.4561]],\n",
            "\n",
            "        [[-3.0631, -4.9115, -4.2817,  ..., -3.3580,  2.0944, -0.8289]],\n",
            "\n",
            "        [[-2.1413, -5.2213, -2.9322,  ...,  1.9783,  0.5546, -3.1189]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.2481, -7.5792, -2.2439,  ..., -3.6613, -3.6689, -1.2272]],\n",
            "\n",
            "        [[-0.3203, -5.4760, -3.4603,  ..., -1.6244, -4.1050, -2.2007]],\n",
            "\n",
            "        [[-1.6799, -7.4093, -3.5864,  ..., -2.2395, -2.7342, -2.4203]]])\n",
            "\n",
            "tgt:\t tensor([ 38,  40, 102, 110,   5,   2,   0,   0,   0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t1.178675\n",
            "pred:\t tensor([[[-1.0950, -4.6185, -4.2589,  ...,  1.7754,  0.9078, -0.7157]],\n",
            "\n",
            "        [[-0.1304, -5.3096, -2.9988,  ..., -2.9299, -0.7518, -0.2369]],\n",
            "\n",
            "        [[-1.2618, -5.6904, -0.8215,  ..., -1.6039,  1.0259, -3.2062]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.9927, -4.6775, -0.8881,  ...,  2.1448, -3.1701, -1.9644]],\n",
            "\n",
            "        [[-2.2052, -4.1358, -4.9130,  ..., -2.8306,  1.6293, -3.5558]],\n",
            "\n",
            "        [[-1.4558, -5.2921, -4.3990,  ...,  1.5243,  0.6096, -1.6109]]])\n",
            "\n",
            "tgt:\t tensor([55,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t1.411413\n",
            "pred:\t tensor([[[-2.5648, -9.0228, -5.8314,  ..., -3.7423, -4.2504, -5.6677]],\n",
            "\n",
            "        [[-3.5011, -4.1219, -2.7117,  ...,  0.7418,  1.4415, -1.3999]],\n",
            "\n",
            "        [[-3.0662, -6.8524, -6.9517,  ..., -0.3650, -1.5660, -2.8751]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.7802, -6.4116, -3.4773,  ..., -1.7871, -1.7890, -1.9710]],\n",
            "\n",
            "        [[-1.7593, -6.9842, -1.9011,  ...,  0.4994, -2.8458, -5.1548]],\n",
            "\n",
            "        [[-1.9636, -3.7993, -4.3615,  ..., -0.5788,  0.8209, -3.9278]]])\n",
            "\n",
            "tgt:\t tensor([52,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t1.320159\n",
            "pred:\t tensor([[[-3.1955e+00, -8.1253e+00, -3.8992e+00,  ..., -3.5778e+00,\n",
            "          -5.2531e-01, -2.1099e+00]],\n",
            "\n",
            "        [[-4.7887e-01, -7.1278e+00, -2.3260e+00,  ...,  1.5740e+00,\n",
            "          -5.1028e+00, -4.9582e+00]],\n",
            "\n",
            "        [[-2.4603e+00, -5.5551e+00, -2.1283e+00,  ..., -2.2421e+00,\n",
            "          -3.2046e+00, -6.8462e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.3591e+00, -5.6886e+00, -6.0395e-01,  ..., -6.9230e-01,\n",
            "          -1.2080e+00, -3.9168e+00]],\n",
            "\n",
            "        [[-2.5932e+00, -7.5209e+00, -1.4071e+00,  ..., -3.5740e+00,\n",
            "           5.4966e-03, -3.1129e+00]],\n",
            "\n",
            "        [[-4.5205e-01, -5.5509e+00, -1.9857e+00,  ..., -5.3702e-01,\n",
            "          -5.0166e+00, -2.7025e+00]]])\n",
            "\n",
            "tgt:\t tensor([48,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t1.185420\n",
            "pred:\t tensor([[[-1.6211, -8.0803, -1.8034,  ..., -1.8197, -2.2973, -4.8694]],\n",
            "\n",
            "        [[-1.7312, -4.6452, -5.1554,  ...,  0.8010,  0.5700, -2.9073]],\n",
            "\n",
            "        [[-2.3139, -5.5811, -3.6538,  ..., -1.7083, -0.3039, -3.3744]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.5799, -4.8466, -3.8341,  ...,  0.2200, -1.7576, -5.0274]],\n",
            "\n",
            "        [[-2.0342, -4.9506, -2.1755,  ...,  1.4125,  0.0553, -4.4433]],\n",
            "\n",
            "        [[-0.6678, -5.4891, -1.1215,  ..., -1.8490,  3.1782, -2.0780]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 33, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t0.802791\n",
            "pred:\t tensor([[[-0.9737, -4.9476, -5.0765,  ..., -0.4568, -0.5951, -1.2913]],\n",
            "\n",
            "        [[ 1.1198, -6.2359, -0.2960,  ..., -1.2867, -4.4422, -5.1748]],\n",
            "\n",
            "        [[-2.8437, -8.1520, -5.6059,  ...,  0.6465, -0.9548, -4.6865]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.2818, -5.0752, -0.2902,  ..., -0.8217,  1.4517, -1.8442]],\n",
            "\n",
            "        [[ 0.1408, -7.8144, -4.3184,  ..., -1.5849, -3.8075, -3.9655]],\n",
            "\n",
            "        [[ 1.0071, -5.9478, -1.0024,  ..., -2.6415, -5.5445, -3.7489]]])\n",
            "\n",
            "tgt:\t tensor([199, 108,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t1.627087\n",
            "pred:\t tensor([[[-4.4459, -7.9378, -6.5320,  ..., -1.1946, -2.7242, -3.7990]],\n",
            "\n",
            "        [[-2.3041, -7.3025, -5.2171,  ..., -3.6514, -0.8492, -6.1532]],\n",
            "\n",
            "        [[-3.3588, -7.2502, -3.8597,  ..., -3.8078,  1.6576, -1.4509]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.7126, -5.7315, -2.5372,  ..., -0.5813, -3.1506, -2.1305]],\n",
            "\n",
            "        [[-1.1330, -7.3977, -1.7149,  ..., -2.0659, -2.8170, -4.5952]],\n",
            "\n",
            "        [[-2.3445, -7.2463, -3.3415,  ..., -1.7880, -1.7969, -4.9311]]])\n",
            "\n",
            "tgt:\t tensor([108,   3,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t1.310779\n",
            "pred:\t tensor([[[-1.1232, -7.5739, -6.4788,  ...,  2.0162, -3.6235, -2.3699]],\n",
            "\n",
            "        [[-1.8208, -4.6816, -5.0720,  ...,  0.9579,  0.7920, -2.3893]],\n",
            "\n",
            "        [[-2.4930, -6.8245, -5.7295,  ...,  0.8811, -2.0446, -1.5246]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.8411, -5.8464, -1.7157,  ..., -1.9069, -0.6467, -2.5768]],\n",
            "\n",
            "        [[-3.0771, -5.0719, -1.5232,  ..., -0.0623,  0.8151, -0.7315]],\n",
            "\n",
            "        [[-3.0606, -9.3561, -5.5941,  ..., -0.6538, -5.7602, -4.3177]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 74,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t1.229805\n",
            "pred:\t tensor([[[-2.6367, -4.8704, -2.7015,  ...,  0.5409,  0.6780, -3.0514]],\n",
            "\n",
            "        [[-0.9993, -5.1263, -0.6557,  ..., -2.0644, -1.4704, -2.1921]],\n",
            "\n",
            "        [[ 0.3296, -8.8282, -4.2773,  ..., -1.9023, -4.0971, -4.0509]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-3.1003, -6.8827, -7.2981,  ..., -0.0223, -2.5040, -2.3822]],\n",
            "\n",
            "        [[-0.0861, -8.3573, -3.9854,  ...,  0.2045, -8.8496, -6.3778]],\n",
            "\n",
            "        [[ 0.1025, -6.9051, -3.8401,  ...,  2.1976, -6.3604, -5.9342]]])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t0.924190\n",
            "pred:\t tensor([[[-3.9438, -8.4682, -6.2370,  ..., -2.6665, -4.2066, -5.6716]],\n",
            "\n",
            "        [[-3.8334, -5.8246, -1.2417,  ..., -1.0124, -0.8744, -0.8244]],\n",
            "\n",
            "        [[-1.5313, -5.0656, -1.3146,  ..., -1.2262,  1.2488, -2.1534]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.3543, -9.2736, -1.8146,  ..., -1.6521, -6.1162, -6.0160]],\n",
            "\n",
            "        [[-2.3175, -7.4323, -3.9985,  ...,  0.0242, -3.3184, -5.3664]],\n",
            "\n",
            "        [[-1.6252, -6.1646, -3.7629,  ...,  1.6948, -3.0207, -4.9024]]])\n",
            "\n",
            "tgt:\t tensor([52, 40,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t1.073768\n",
            "pred:\t tensor([[[-1.3877, -7.9814, -6.0462,  ..., -1.5949, -5.1135, -4.6116]],\n",
            "\n",
            "        [[-5.5910, -8.8785, -3.0156,  ..., -4.2971,  0.5139, -4.2373]],\n",
            "\n",
            "        [[ 0.3804, -3.6742, -2.5161,  ..., -1.2796, -2.2413, -1.4139]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.9371, -4.3585, -2.4511,  ...,  1.8990, -1.6730, -5.6302]],\n",
            "\n",
            "        [[-0.8645, -7.2188, -1.1511,  ..., -2.4463, -2.8500, -3.7670]],\n",
            "\n",
            "        [[-4.6099, -6.0270, -6.5142,  ..., -2.6673, -3.0130, -4.1538]]])\n",
            "\n",
            "tgt:\t tensor([36,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t1.076834\n",
            "pred:\t tensor([[[-0.1423, -8.2614, -0.8760,  ..., -1.1200, -5.6080, -4.0192]],\n",
            "\n",
            "        [[ 0.5261, -7.7074, -4.0564,  ...,  0.1670, -5.1470, -3.0364]],\n",
            "\n",
            "        [[-1.6573, -7.0915, -5.1714,  ..., -0.7821, -4.3823, -5.9615]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1562, -5.8949, -1.2162,  ..., -2.5236, -6.4595, -7.2161]],\n",
            "\n",
            "        [[-2.9660, -4.6897, -0.2472,  ...,  0.5973,  1.8634, -1.9790]],\n",
            "\n",
            "        [[-2.2728, -8.0179, -3.4094,  ...,  1.3598, -1.8871, -3.3204]]])\n",
            "\n",
            "tgt:\t tensor([ 3, 24,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_lstm(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `lstm_net` variable.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "lr = 1e-3\n",
        "# END OF YOUR CODE\n",
        "epochs = 50\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "lstm_net = NMTLSTM(vocab_eng.num_word, vocab_fra.num_word, embedding_dim, hidden_size, device)\n",
        "\n",
        "lstm_loss_list = train_lstm(lstm_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQSbkq_aFPKK"
      },
      "source": [
        "### LSTM Loss Curve\n",
        "\n",
        "Plot the loss curve over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UMekDZEtFPKK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "b825cb75-5582-4024-e69a-7f91799cf004"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of LSTM Attention')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if device != \"cpu\":\n",
        "    lstm_loss_list_cpu = [ele.cpu() for ele in lstm_loss_list]\n",
        "else:\n",
        "    lstm_loss_list_cpu = lstm_loss_list\n",
        "plt.plot(np.arange(len(lstm_loss_list_cpu)), lstm_loss_list_cpu)\n",
        "plt.title('Loss Curve of LSTM Attention')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmBwQqm3FPKK"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 80% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ab1KNBxiFPKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ba06ae-be4f-4eed-b055-97011242d66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['entrez', '!', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', \"t'en\", 'dois', 'une', '.', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'mort', '?', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['demande', 'à', 'qui', 'que', 'ce', 'soit', '!', 'pad']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.9497\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "\n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "\n",
        "def evaluate_lstm(net, train_iter, device):\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()]\n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "evaluate_lstm(lstm_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoN_xDrSFPKL"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "Recurrent Neural Networks can capture long-range, variable-length sequential information, but updating the current state relies on the previous states. Thus it cannot be parallelized across the entire sequence. In contrast, CNNs are easy to parallelize but they cannot capture sequential dependency within variable-length sequences and their receptive field is limited. Transformers resolve this dilemna by being able to capture long-range dependencies while also being easy to parallelize.\n",
        "\n",
        "Transformers consist of several different components and we will walk you through each of them. The original paper can be found [here](https://arxiv.org/pdf/1706.03762.pdf). [Here](http://jalammar.github.io/illustrated-transformer/) is a very informative blog about transformers that should also be a good reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmK1cu2vFPKL"
      },
      "source": [
        "**Multi-Head Self-Attention**\n",
        "\n",
        "Multi-head self-attention is an extension of the dot-product attention we've previously implemented. The \"self-attention\" part means that the query, key, and value all come from the same sequence. For a sentence, this means that we are looking at how each word pays attention to other words in the same sentence. The \"multi-head\" part means instead of only having one attention map, we can have multiple. This means that for a given word in the sentence, it can pay attention to different parts of the sentence.\n",
        "\n",
        "The steps in the multi-head attention can be summarzied by the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktnP21X2FPKL"
      },
      "source": [
        "   1. The multi-head self-attention takes the initial query $Q$, key $K$, and value $V$ as input. Note that, if not provided specifically, usually these are set to the same input embeddings $X=Q=K=V$ initially.\n",
        "   \n",
        "   1. Then, a linear projection is applied to $Q,K,V$ sepearately for each head $i=1,\\dots,h$.\n",
        "      $$\n",
        "   Q_i = QW^{Q}_i, K_i = KW^{K}_i, V_i = VW^{V}_i, i \\in [0, \\dots, h-1],\n",
        "   $$\n",
        "   \n",
        "   where $W^Q_i \\in \\mathcal{R}^{d_{model} \\times d_k}, W^K_i \\in \\mathcal{R}^{d_{model} \\times d_k}\\text{, and } W^V_i \\in \\mathcal{R}^{d_{model} \\times d_v}$.\n",
        "     \n",
        "   1. Apply the scaled dot-product attention to each of these projected set of queries, keys, and values:\n",
        "   $$\n",
        "   \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}(\\frac{Q_iK^T_i}{\\sqrt{d_k}})V_i\n",
        "   $$\n",
        "   \n",
        "   1. Concatenate all the heads together and project it with another learned linear projections:\n",
        "   \n",
        "   $$\n",
        "   \\text{O} = \\text{Concate(head}_1, \\dots, \\text{head}_h) \\\\\n",
        "   \\text{MultiHead}(Q, K, V) = \\text{O}W^o, \\hspace{10mm} \\text{where } W^o \\in \\mathcal{R}^{{hd_v} \\times d_{model}}\n",
        "   $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7keposqeFPKM"
      },
      "source": [
        "A good visualization from the above referenced [blog](http://jalammar.github.io/illustrated-transformer/) is shown below. Transformer stacks several multi-head attention modules together. For the first multi-head layer, the input is from the dataset, so an additional embedding layer is needed to project the input sequence into the appropriate dimensions. For subsequent layers, the output from the layer previous layer is directly used as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kn4Yjl6FPKM"
      },
      "source": [
        "<div>\n",
        "    <img src=\"http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrBO8qaRFPKM"
      },
      "source": [
        "Implement the MultiHeadAttention class below:\n",
        " - Complete the __init__() function, where the linear mappings for query, key, values, and output should be created.\n",
        " - Complete the forward() function, where the multi-head attention is performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "deletable": false,
        "id": "8oOFoYPbFPKM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ffb0d0e9e2f0f149afde973ef62dc33f",
          "grade": false,
          "grade_id": "cell-41f2921301bf3992",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, dk, num_heads,  **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, the same d_model in paper, feature dimension of query/key/values\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      num_heads: int, number of heads used for this MultiHeadAttention\n",
        "    \"\"\"\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = DotProductAttention()\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the linear mappings for the query, key, and values.\n",
        "    # Also initialize the weight matrix for the output.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.W_v = nn.Linear(d_model, dk * num_heads, bias = False)\n",
        "    self.W_o = nn.Linear(dk * num_heads, d_model, bias = False)\n",
        "    self.W_q = nn.Linear(d_model, dk * num_heads, bias = False)\n",
        "    self.W_k = nn.Linear(d_model, dk * num_heads, bias = False)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, query, key, value, valid_length):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "      query: tensor of size (B, T, d_model)\n",
        "      key: tensor of size (B, T, d_model)\n",
        "      value: tensor of size (B, T, d_model)\n",
        "      valid_length: (B, )\n",
        "\n",
        "      B is the batch_size, T is length of sequence, d_model is the feature dimensions of query,\n",
        "      key, and value.\n",
        "\n",
        "    Outputs:\n",
        "      attention (B, T, d_model)\n",
        "      \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of MultiHeadAttention.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    def transpose_(input_tensor, num_of_heads, flag):\n",
        "      if flag == 0:\n",
        "        result = input_tensor.reshape(input_tensor.shape[0], input_tensor.shape[1], num_of_heads, -1)\n",
        "        result = result.permute(0, 2, 1, 3)\n",
        "        return result.reshape(-1, result.shape[2], result.shape[3])\n",
        "      else:\n",
        "        result = input_tensor.reshape(-1, num_of_heads, input_tensor.shape[1], input_tensor.shape[2])\n",
        "        result = result.permute(0, 2, 1, 3)\n",
        "        return result.reshape(result.shape[0], result.shape[1], -1)\n",
        "\n",
        "    key = transpose_(self.W_k(key), self.num_heads, 0)\n",
        "    value = transpose_(self.W_v(value), self.num_heads, 0)\n",
        "    temp_query = self.W_q(query)\n",
        "    query = transpose_(temp_query, self.num_heads, 0)\n",
        "\n",
        "    if valid_length != None:\n",
        "      if valid_length.ndim == 1:\n",
        "        valid_length = valid_length.repeat(self.num_heads)\n",
        "      else:\n",
        "        valid_length = valid_length.repeat(self.num_heads, 1)\n",
        "    attention = transpose_(self.attention(query, key, value, valid_length), self.num_heads, 1)\n",
        "    attention = self.W_o(attention)\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "XepbDzptFPKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98fd59b-0017-4a6b-d297-5b2e5b4cef02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "cell = MultiHeadAttention(5, 90, 9)\n",
        "X = torch.ones((2, 4, 5))\n",
        "valid_len = torch.tensor([2, 3])\n",
        "cell(X, X, X, valid_len).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEWFEEwEFPKN"
      },
      "source": [
        "### Position-wise Feed-Forward Network\n",
        "\n",
        "Another key component in the Transformer block is the position-wise feed-forward network (FFN). It's called position-wise FFN because the linear mapping is applied to each position separately and identically. For example, for an embedded input of size $N \\times T \\times D_{in}$, there are $N*T$ vectors of dimension $D_{in}$. If we apply a one layer position-wise FFN with weights of size $D_{in} \\times D_{out}$. The linear projection will be applied to each of the $N*T$ vectors separately and identically. Thus, the output would have size $N \\times T \\times D_{out}$. Another way to think about this is that this is the same as a 1x1 convolution mapping from $D_{in}$ channels to $D_{out}$ channels.\n",
        "\n",
        "Transformers stack two layers of position-wise FFN together, with a ReLU activation in between:\n",
        "\n",
        "$$\n",
        "\\text{PositionWiseFFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\n",
        "$$\n",
        "\n",
        "Complete the class PositionWiseFFN:\n",
        "\n",
        "- Complete the __init__() function, where two position-wise FFN should be created.\n",
        "- Complete the forward() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "id": "MS3Ruu13FPKN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "144103b4a6258c267c6a327e33851042",
          "grade": false,
          "grade_id": "cell-39ca6d33ccf1a8f5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class PositionWiseFFN(nn.Module):\n",
        "  def __init__(self, input_size, ffn_l1_size, ffn_l2_size):\n",
        "    super(PositionWiseFFN, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      input_size: int, feature dimension of the input\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize the feed forward network for PositionWiseFFN\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.relu = nn.ReLU()\n",
        "    self.lin1 = nn.Linear(input_size, ffn_l1_size)\n",
        "    self.lin2 = nn.Linear(ffn_l1_size, ffn_l2_size)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      X: tensor of size (N, T, D_in)\n",
        "    Output:\n",
        "      o: tensor of size (N, T, D_out)\n",
        "    \"\"\"\n",
        "    o = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass of PositionWiseFFN\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    o = self.lin1(X)\n",
        "    o = self.relu(o)\n",
        "    o = self.lin2(o)\n",
        "    # END OF YOUR CODE\n",
        "    return o\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgePpzNFFPKO"
      },
      "source": [
        "Check your result. Expected output\n",
        "\n",
        "```\n",
        "[[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
        "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
        "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7PNTreRXFPKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08699909-8ef5-4fcf-9ec1-0220ae6ab07b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
              "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879],\n",
              "        [ 0.1609,  0.0371,  0.4916,  0.1781,  0.2010,  0.0161,  0.0869, -0.1879]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "seed(1)\n",
        "ffn = PositionWiseFFN(4, 4, 8)\n",
        "ffn(torch.ones((2, 3, 4)))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwNTph3BFPKP"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Replacing RNNs with the multi-head attention layer and applying the position-wise feed-forward network makes the computation parallelizable since these modules compute the output of each item in the sequence independently. However, since every item is processed in parallel, there is no notion of ordering of the sequence. For an input sentence, this means that the transformer doesn't know the ordering of the words in the sentence. For most tasks, this ordering is very important. To address this, transformers propose adding a positional encoding to each input that corresponds to the position in the sequence. This means that we take the position of each word in the sentence (eg. 0, 1, 2, etc...) and map it to some $d_{model}$-dimensional embedding. We then add this embedding with every input item so that the input is not position-aware. Transformers use the following sinusoidal positional encoding:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "PE_{(pos, 2i)} &= sin(pos / 10000^{2i/d_{model}}) \\\\\n",
        "PE_{(pos, 2i+1)} &= cos(pos / 10000^{2i/d_{model}})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "An example borrowed from this [blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) can give an ituition how this positional encoding works. Suppose you want to encode the number from $0$ to $8$ using binary encoding, the result would like this:\n",
        "$$\n",
        "\\begin{align*}\n",
        "0: && 0  0  0 \\\\\n",
        "1: && 0  0  1 \\\\\n",
        "2: && 0  1  0 \\\\\n",
        "3: && 0  1  1 \\\\\n",
        "4: && 1  0  0 \\\\\n",
        "5: && 1  0  1 \\\\\n",
        "6: && 1  1  0 \\\\\n",
        "7: && 1  1  1 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "Note the frequency of ones in each digit is different. Thus, words at different locations will have different embedding features (digits in the example). The figure below visualized a position encoding matrix of dimension $\\mathcal{R}^{50 \\times 128}$\n",
        "\n",
        "The forward pass of the positional encoding should add the positional embedding to the input:\n",
        "\n",
        "`y = x + positional_encoding(x)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhdNRxqnFPKP"
      },
      "source": [
        "<div>\n",
        "    <img src=\"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECTqtni4FPKQ"
      },
      "source": [
        "Complete the class PositionalEncoding:\n",
        "- Complete the __init__() function, where the tensor $PE$ should be created.\n",
        "- Complete the forward() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "deletable": false,
        "id": "YtTr6b3QFPKQ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6f724034b91387df0b62a10fb7268858",
          "grade": false,
          "grade_id": "cell-fc6933701b990269",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, dim, device, max_len=1000):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      dim: feature dimension of the positional encoding\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Initialize positional encoding. You should create `self.pe`\n",
        "    # here according to the definition above. The positional encoding should\n",
        "    # support up to position `max_len`.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    x = torch.arange(0, max_len, dtype = torch.float32).reshape(-1, 1)\n",
        "    x = x /torch.pow(10000, torch.arange(0, dim, 2, dtype = torch.float32) / dim)\n",
        "    self.p = torch.zeros((1, max_len, dim))\n",
        "    self.p[:, :, 0::2] = torch.sin(x)\n",
        "    self.p[:, :, 1::2] = torch.cos(x)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D_in)\n",
        "    Output:\n",
        "      Y: tensor of the same size of X\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for positional encoding. After getting the positional\n",
        "    # encoding with regards to the time dimension, add it to the input X.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    Y = self.p[:, :X.shape[1], :].to(X.device)\n",
        "    Y = X + Y\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmUgzqgxFPKQ"
      },
      "source": [
        "Check your result. Expected output\n",
        "\n",
        "```\n",
        "tensor([[1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000,\n",
        "         2.0000],\n",
        "        [1.8415, 1.5403, 1.1578, 1.9875, 1.0251, 1.9997, 1.0040, 2.0000, 1.0006,\n",
        "         2.0000],\n",
        "        [1.9093, 0.5839, 1.3117, 1.9502, 1.0502, 1.9987, 1.0080, 2.0000, 1.0013,\n",
        "         2.0000],\n",
        "        [1.1411, 0.0100, 1.4578, 1.8891, 1.0753, 1.9972, 1.0119, 1.9999, 1.0019,\n",
        "         2.0000],\n",
        "        [0.2432, 0.3464, 1.5923, 1.8057, 1.1003, 1.9950, 1.0159, 1.9999, 1.0025,\n",
        "         2.0000]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "k76uE2IMFPKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea3f5c4-ccb2-43b8-da00-750673526a92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000, 2.0000, 1.0000,\n",
              "         2.0000],\n",
              "        [1.8415, 1.5403, 1.1578, 1.9875, 1.0251, 1.9997, 1.0040, 2.0000, 1.0006,\n",
              "         2.0000],\n",
              "        [1.9093, 0.5839, 1.3117, 1.9502, 1.0502, 1.9987, 1.0080, 2.0000, 1.0013,\n",
              "         2.0000],\n",
              "        [1.1411, 0.0100, 1.4578, 1.8891, 1.0753, 1.9972, 1.0119, 1.9999, 1.0019,\n",
              "         2.0000],\n",
              "        [0.2432, 0.3464, 1.5923, 1.8057, 1.1003, 1.9950, 1.0159, 1.9999, 1.0025,\n",
              "         2.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "seed(1)\n",
        "pe = PositionalEncoding(10, device)\n",
        "pe(torch.ones((2, 5, 10), device=device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2INK-oFPKQ"
      },
      "source": [
        "### Add and Norm\n",
        "\n",
        "Transformers use a residual connection followed by a layer normalization layer to connect the inputs and outputs of other layers. To be specific, an \"add and norm\" layer is appended after each multi-head attention layer and the position-wise FFN layer. *The code for AddNorm Layer is given as below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GpaWYcXIFPKQ"
      },
      "outputs": [],
      "source": [
        "class AddNorm(nn.Module):\n",
        "    def __init__(self, dropout, embedding_size):\n",
        "        super(AddNorm, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(embedding_size)\n",
        "\n",
        "    def forward(self, X, Y):\n",
        "        return self.norm(self.dropout(Y) + X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzezqMGoFPKQ"
      },
      "source": [
        "### Encoder and Decoder\n",
        "\n",
        "The following figure gives a simple example of how the Transformer is built on these components introduced above. It's easy to see that the encoder of the Transformer consists of several identical encoder blocks, and so does the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyL4sXFMFPKR"
      },
      "source": [
        "<div>\n",
        "<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" width=\"600\"/>\n",
        "</div>\n",
        "\n",
        "Image source: http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgnnUj26FPKR"
      },
      "source": [
        "Complete the forward() function for the EncoderBlock and DecoderBlock. Note that, for the decoder, when applying self-attention, the sequential queries **cannot** attend to those at later time steps. For example, in a sequence, the query entry at time step 5 can only observe the first 5 entries. You can use `valid_length` to enforce this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "deletable": false,
        "id": "pzSHqNVFFPKS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a74c7336c9f0f3646b41cd064e1ebadf",
          "grade": false,
          "grade_id": "cell-544ed4c7777ee925",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads, dropout):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "\n",
        "    \"\"\"\n",
        "    self.attention = MultiHeadAttention(d_model, d_k, num_heads)\n",
        "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
        "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
        "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
        "\n",
        "  def forward(self, X, valid_length):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      valid_length: tensor of size (N), valid lengths for each sequence\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the EncoderBlock. Use the figure above\n",
        "    # for guidance:\n",
        "    # attention -> add+norm -> feed forward -> add+norm\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))\n",
        "    Y = self.addnorm_2(Y, self.ffn(Y))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "deletable": false,
        "id": "sVHRK0SmFPKS",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "77dbfb80f87e8bc1beb98efaa7c01e61",
          "grade": false,
          "grade_id": "cell-8d2fafea68ccbc9f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, d_k, ffn_l1_size, ffn_l2_size, num_heads,\n",
        "             dropout, **kwargs):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      d_k: int, feature projected dimension of query/key/value, we follow the setting in the paper, where d_v=d_k=d_q\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "\n",
        "    \"\"\"\n",
        "    self.attention_1 = MultiHeadAttention(d_model, d_k, num_heads)\n",
        "    self.addnorm_1 = AddNorm(dropout, d_model)\n",
        "    self.attention_2 = MultiHeadAttention(d_model, d_model, num_heads)\n",
        "    self.addnorm_2 = AddNorm(dropout, d_model)\n",
        "    self.ffn = PositionWiseFFN(d_model, ffn_l1_size, ffn_l2_size)\n",
        "    self.addnorm_3 = AddNorm(dropout, d_model)\n",
        "\n",
        "  def forward(self, X, **kwargs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      **kwargs: other arguments you think is necessary for implementation\n",
        "    Outputs:\n",
        "      Y: tensor of size (N, T, D_out)\n",
        "\n",
        "      Feel free to output variables if necessary.\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the DecoderBlock. Use the figure above\n",
        "    # for guidance:\n",
        "    # self attention -> add+norm -> enc-dec attention -> add+norm -> feed forward -> add+norm\n",
        "    # for the first attention layer, make sure to construct a `valid_length` that\n",
        "    # ensures each element cannot attend to later elements in the sequence.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    state = kwargs['state']\n",
        "    valid_length_encoder = state[0]\n",
        "\n",
        "    size_of_batch, length_seqnce, _ = X.shape\n",
        "    valid_len = torch.arange(1, length_seqnce + 1, device = X.device).repeat(size_of_batch, 1)\n",
        "    tem1 = self.attention_1(X, X, X, valid_len)\n",
        "    n1 = self.addnorm_1(X, tem1)\n",
        "    tem2 = self.attention_2(n1, n1, n1, valid_length_encoder)\n",
        "    n2 = self.addnorm_2(n1, tem2)\n",
        "    Y = self.addnorm_3(n2, self.ffn(n2))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sco9Z0_XFPKT"
      },
      "source": [
        "### Transformer  Implementation\n",
        "\n",
        "By stacking two encoder blocks and two decoder blocks, build the Transformer using the above components.\n",
        "\n",
        "- Implement the Encoder of Transformer:\n",
        " - Complete the __init__() function with a word embedding layer and several EncoderBlocks.\n",
        " - Complete the forward() function\n",
        "- Implement the Decoder of Transformer\n",
        " - Complete the __init__() function\n",
        " - Complete the forward() function\n",
        "- Implement the Transformer\n",
        " - Complete the forward() function\n",
        " - Complete the predict() function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "deletable": false,
        "id": "jBq7fk-uFPKT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ae68e2f2cb0af15282eeaa8ff420a15c",
          "grade": false,
          "grade_id": "cell-33857dd68f665a2b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
        "               num_heads, num_layers, dropout, device):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      num_layers: number of encoder blocks\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement init() function for TransformerEncoder. See forward() notes\n",
        "    # for more details.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.num_layers = num_layers\n",
        "    self.pos_enc = PositionalEncoding(d_model, dropout)\n",
        "    self.d_model = d_model\n",
        "    self.enc_blks = nn.Sequential()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      self.enc_blks.add_module(\"encoder block \" + str(i), EncoderBlock(d_model, d_model//num_heads, ffn_l1_size, ffn_l2_size, num_heads, dropout))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "  def forward(self, X, valid_length):\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the TransformerEncoder\n",
        "    # First, use an embedding so each element in X is d_model (hint: use nn.Embedding)\n",
        "    # Then, apply the positional embedding to each element\n",
        "    # Lastly, pass the resulting input into num_layers of EncoderBlocks\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    X = self.pos_enc(self.embedding(X) * (self.d_model**0.5))\n",
        "    for bloc in self.enc_blks:\n",
        "        X = bloc(X, valid_length)\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "deletable": false,
        "id": "AADZuNQrFPKU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "765b53867f3968948b51990b9c69f80f",
          "grade": false,
          "grade_id": "cell-eefed3b5de8d2f3d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, ffn_l1_size, ffn_l2_size,\n",
        "             num_heads, num_layers, dropout, device):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      d_model: int, feature dimension of query/key/value\n",
        "      fnn_l1_size: int, feature dimension of the output after the first position-wise FFN.\n",
        "      fnn_l2_size: int, feature dimension of the output after the second position-wise FFN.\n",
        "      num_heads: int, number of head for multi-head attention layer.\n",
        "      dropout: dropout probability for dropout layer.\n",
        "      num_layers: number of decoder blocks\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement init() function for TransformerDecoder\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hiddens = d_model\n",
        "    self.embedding = nn.Embedding(vocab_size, self.num_hiddens)\n",
        "    self.pos_enc = PositionalEncoding(self.num_hiddens, dropout)\n",
        "    self.dec_blks = nn.Sequential()\n",
        "    self.dense = nn.Linear(self.num_hiddens, vocab_size)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      self.dec_blks.add_module(\"encoder block \" + str(i), DecoderBlock(d_model, d_model, ffn_l1_size, ffn_l2_size, num_heads, dropout))\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "\n",
        "  def forward(self, X, state):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      X: tensor of size (N, T, D), embedded input sequences\n",
        "      valid_length: tensor of size (N,), valid lengths for each sequence\n",
        "    \"\"\"\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass for the TransformerDecoder. This will look\n",
        "    # very similar to the TransformerEncoder.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    kwargs = {'state':state}\n",
        "    X = self.pos_enc(self.embedding(X) * (self.num_hiddens**0.5))\n",
        "\n",
        "    for bloc in self.dec_blks:\n",
        "        X = bloc(X, **kwargs)\n",
        "    # END OF YOUR CODE\n",
        "    return self.dense(X), X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "deletable": false,
        "id": "gCQ_p6ccFPKU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "68446581beb8f1b29dcb65023d32bcdf",
          "grade": false,
          "grade_id": "cell-1a06f1a890069bc5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
        "  def __init__(self, encoder, decoder, **kwargs):\n",
        "    super(Transformer, self).__init__(**kwargs)\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
        "    \"\"\"Forward function\"\"\"\n",
        "    loss = 0\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement forward pass of transformer\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    state = (src_valid_len, self.encoder(src_array, src_valid_len))\n",
        "    pred, _ = self.decoder(tgt_array, state)\n",
        "    T = tgt_array.shape[1]\n",
        "    for i in range(0, T-1):\n",
        "      loss += F.nll_loss(F.log_softmax(pred[:, i]), tgt_array[:, i+1])\n",
        "    pred = pred.argmax(dim = -1)\n",
        "    # END OF YOUR CODE\n",
        "    return loss, pred\n",
        "\n",
        "  def predict(self, src_array, src_valid_len, tgt_array, tgt_valid_len):\n",
        "    pred = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement predict() of transformer\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pred = torch.clone(tgt_array)\n",
        "    state = []\n",
        "    state.append(src_valid_len)\n",
        "    state.append(self.encoder(src_array, src_valid_len))\n",
        "\n",
        "    for i in range(0,MAX_LEN-1):\n",
        "      x_t, _ = self.decoder(pred, state)\n",
        "      x_t = x_t.argmax(dim = -1)\n",
        "      pred[:, i] = x_t[:, i]\n",
        "\n",
        "    pred = pred[:, :-1]\n",
        "    # END OF YOUR CODE\n",
        "    return pred\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL-YquNmFPKU"
      },
      "source": [
        "Find a good learning rate for training this model. Feel free to tune other hyperparameters as well as long as your best model is saved in `transformer_net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "deletable": false,
        "id": "L-F6jPOkFPKV",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4caf1154ac2fe22f1709fdf9e258be88",
          "grade": false,
          "grade_id": "cell-535ea1c455d3fe85",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e530cbb-11fc-4b23-836c-75e0abe2ba86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-4fef816cfbcf>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  loss += F.nll_loss(F.log_softmax(pred[:, i]), tgt_array[:, i+1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0 / 7800\tLoss:\t55.256775\n",
            "pred:\t tensor([ 24, 119, 255, 254, 332, 402, 246, 195, 195, 193])\n",
            "\n",
            "tgt:\t tensor([ 48,   3, 114,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 156 / 7800\tLoss:\t3.934302\n",
            "pred:\t tensor([ 48, 164,   3,  48,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 48, 164, 125,  48,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 312 / 7800\tLoss:\t2.022877\n",
            "pred:\t tensor([ 9, 80,  3,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([ 9, 80,  3,  5,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 468 / 7800\tLoss:\t1.257010\n",
            "pred:\t tensor([250, 207,   3,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([250, 207,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 624 / 7800\tLoss:\t1.286495\n",
            "pred:\t tensor([111, 386,   5,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([111, 386,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 780 / 7800\tLoss:\t0.975977\n",
            "pred:\t tensor([99,  3, 11,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([99,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 936 / 7800\tLoss:\t0.858864\n",
            "pred:\t tensor([ 48, 164,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 48, 164,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 1092 / 7800\tLoss:\t0.953923\n",
            "pred:\t tensor([ 14, 116, 179, 259,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 116, 179,  34,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1248 / 7800\tLoss:\t0.658811\n",
            "pred:\t tensor([14, 79, 28, 41,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([14, 79, 28, 41, 84, 11,  2,  0,  0])\n",
            "\n",
            "iter 1404 / 7800\tLoss:\t0.348326\n",
            "pred:\t tensor([145, 272,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([145, 272,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 1560 / 7800\tLoss:\t0.522981\n",
            "pred:\t tensor([155, 357, 254,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([155, 357, 254,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 1716 / 7800\tLoss:\t0.625980\n",
            "pred:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 1872 / 7800\tLoss:\t1.023487\n",
            "pred:\t tensor([ 46, 338, 194, 207,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 46, 338, 194, 207,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2028 / 7800\tLoss:\t0.400774\n",
            "pred:\t tensor([48, 49,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([48, 49,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2184 / 7800\tLoss:\t0.628990\n",
            "pred:\t tensor([142, 328,   5,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([142, 328,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2340 / 7800\tLoss:\t0.536878\n",
            "pred:\t tensor([36, 74,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([36, 74,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 2496 / 7800\tLoss:\t0.665207\n",
            "pred:\t tensor([15, 72,  3, 81,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([ 15,  72, 259,  81,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 2652 / 7800\tLoss:\t0.679514\n",
            "pred:\t tensor([ 14,  17, 287,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14,  17, 287,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 2808 / 7800\tLoss:\t1.135406\n",
            "pred:\t tensor([142, 226,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([142, 226,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 2964 / 7800\tLoss:\t0.980514\n",
            "pred:\t tensor([38, 40,  3, 11, 11, 11,  2,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([ 38,  40,   3,   3, 261,  11,   2,   0,   0])\n",
            "\n",
            "iter 3120 / 7800\tLoss:\t0.219868\n",
            "pred:\t tensor([53, 58,  5,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([53, 58,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 3276 / 7800\tLoss:\t1.013285\n",
            "pred:\t tensor([ 79, 171,   3,   9,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 79, 171,   3,  41,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3432 / 7800\tLoss:\t0.576688\n",
            "pred:\t tensor([177,   3,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([177,   3,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 3588 / 7800\tLoss:\t0.830306\n",
            "pred:\t tensor([ 14, 375,  41,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 375,  41, 240,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 3744 / 7800\tLoss:\t0.485820\n",
            "pred:\t tensor([ 14, 211, 310,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 211, 310,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 3900 / 7800\tLoss:\t0.685126\n",
            "pred:\t tensor([48,  3,  3, 11,  2,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([48,  3,  3, 11,  2,  0,  0,  0,  0])\n",
            "\n",
            "iter 4056 / 7800\tLoss:\t0.714840\n",
            "pred:\t tensor([176, 203,  14,  70, 289,  24,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([176, 203,  14,  70, 289,  24,   2,   0,   0])\n",
            "\n",
            "iter 4212 / 7800\tLoss:\t0.577570\n",
            "pred:\t tensor([ 14, 201, 146,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 201, 146,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4368 / 7800\tLoss:\t0.074787\n",
            "pred:\t tensor([ 62, 151,   5,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 62, 151,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4524 / 7800\tLoss:\t0.508214\n",
            "pred:\t tensor([68,  5,  2,  0,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([68,  5,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "iter 4680 / 7800\tLoss:\t0.216194\n",
            "pred:\t tensor([ 14, 201,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 201,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 4836 / 7800\tLoss:\t0.927375\n",
            "pred:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([251, 354,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 4992 / 7800\tLoss:\t0.173860\n",
            "pred:\t tensor([212, 219,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([212, 219,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5148 / 7800\tLoss:\t0.757452\n",
            "pred:\t tensor([ 14, 211, 264,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 211, 264,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5304 / 7800\tLoss:\t0.294622\n",
            "pred:\t tensor([251,  75,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([251,  75,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5460 / 7800\tLoss:\t0.238066\n",
            "pred:\t tensor([109,  36,  57,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([109,  36,  57,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5616 / 7800\tLoss:\t0.486422\n",
            "pred:\t tensor([ 92, 341,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 92, 341,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 5772 / 7800\tLoss:\t0.344729\n",
            "pred:\t tensor([266,  85,  24,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([266,  85,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 5928 / 7800\tLoss:\t0.612320\n",
            "pred:\t tensor([109,  36,   3,   5,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([109,  36,   3,   5,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6084 / 7800\tLoss:\t0.707013\n",
            "pred:\t tensor([ 14, 112,  28,   3,   3,  11,   2,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 112,  28,  34,   3,  11,   2,   0,   0])\n",
            "\n",
            "iter 6240 / 7800\tLoss:\t0.716479\n",
            "pred:\t tensor([3, 3, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([3, 3, 5, 2, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 6396 / 7800\tLoss:\t0.598341\n",
            "pred:\t tensor([ 52, 170, 223,  24,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 52, 170, 223,  24,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6552 / 7800\tLoss:\t0.421403\n",
            "pred:\t tensor([ 62, 116, 182,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 62, 116, 182,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 6708 / 7800\tLoss:\t0.196328\n",
            "pred:\t tensor([ 67, 242,  11,   2,   0,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 67, 242,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "iter 6864 / 7800\tLoss:\t0.580501\n",
            "pred:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([171, 342,   3,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7020 / 7800\tLoss:\t1.151784\n",
            "pred:\t tensor([ 14,   3,  37, 298,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([14,  3, 37,  3, 11,  2,  0,  0,  0])\n",
            "\n",
            "iter 7176 / 7800\tLoss:\t0.786056\n",
            "pred:\t tensor([ 14, 211,  72, 252,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 14, 211,  72, 252,  11,   2,   0,   0,   0])\n",
            "\n",
            "iter 7332 / 7800\tLoss:\t0.712625\n",
            "pred:\t tensor([ 38, 358, 117,  11,   2,   0,   0,   0,   0,   0])\n",
            "\n",
            "tgt:\t tensor([ 38, 358, 117,  11,   2,   0,   0,   0,   0])\n",
            "\n",
            "iter 7488 / 7800\tLoss:\t0.389044\n",
            "pred:\t tensor([4, 5, 2, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "tgt:\t tensor([4, 5, 2, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "iter 7644 / 7800\tLoss:\t0.643624\n",
            "pred:\t tensor([55, 56,  5,  2,  0,  0,  0,  0,  0,  0])\n",
            "\n",
            "tgt:\t tensor([55, 56,  5,  2,  0,  0,  0,  0,  0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def train_transformer(net, train_iter, lr, epochs, device):\n",
        "  # training\n",
        "  net = net.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_list = []\n",
        "  print_interval = len(train_iter)\n",
        "  total_iter = epochs * len(train_iter)\n",
        "  for e in range(epochs):\n",
        "    net.train()\n",
        "    for i, train_data in enumerate(train_iter):\n",
        "      train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "      loss, pred = net(*train_data)\n",
        "\n",
        "      loss_list.append(loss.mean().detach())\n",
        "      optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      step = i + e * len(train_iter)\n",
        "      if step % print_interval == 0:\n",
        "        print('iter {} / {}\\tLoss:\\t{:.6f}'.format(step, total_iter, loss.mean().detach()))\n",
        "        print('pred:\\t {}\\n'.format(pred[0].detach().cpu()))\n",
        "        print('tgt:\\t {}\\n'.format(train_data[2][0][1:].cpu()))\n",
        "  return loss_list\n",
        "\n",
        "\n",
        "# hyper-params: feel free to modify the values and numbers of hyper-params\n",
        "\n",
        "# training\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "embedding_dim = 250\n",
        "hidden_size = 128\n",
        "\n",
        "#transformer hp\n",
        "d_model = 512 # for GPU training\n",
        "# d_model = 64 # for CPU training\n",
        "ffn_l1_size = 2048 # for GPU training\n",
        "# ffn_l1_size = 128 # for CPU training\n",
        "ffn_l2_size = d_model\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "\n",
        "lr = None\n",
        "##############################################################################\n",
        "# TODO: Find a good learning rate to train this model. Make sure your best\n",
        "# model is saved to the `transformer_net` variable. Feel free to tune other hyperparameters\n",
        "# as well.\n",
        "##############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "lr = 1e-4\n",
        "# END OF YOUR CODE\n",
        "epochs = 50\n",
        "\n",
        "encoder = TransformerEncoder(vocab_eng.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
        "                             num_heads, num_layers, dropout, device=device)\n",
        "decoder = TransformerDecoder(vocab_fra.num_word, d_model, ffn_l1_size, ffn_l2_size,\n",
        "                             num_heads, num_layers, dropout, device=device)\n",
        "transformer_net = Transformer(encoder, decoder)\n",
        "\n",
        "transformer_loss_list = train_transformer(transformer_net, train_iter, lr, epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MWT7PnWNFPKV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "27e5f313-c3cc-486c-c02f-94dd65007952"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curve of Transformer')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "if device != \"cpu\":\n",
        "    transformer_loss_list_cpu = [ele.cpu() for ele in transformer_loss_list]\n",
        "else:\n",
        "    transformer_loss_list_cpu = transformer_loss_list\n",
        "plt.plot(np.arange(len(transformer_loss_list_cpu)), transformer_loss_list_cpu)\n",
        "plt.title('Loss Curve of Transformer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZoUuny1FPKW"
      },
      "source": [
        "Test the accuracy of your model. You should be able to get at least 90% accuracy if you use GPU (with a larger hidden dimension) and at least 45% if you use CPU (with a smaller hidden dimension)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vE366kKRFPKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a03e346-f7d9-44c3-bf8e-4cb28d8dad10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4, 5, 0, 0, 0, 0, 0, 0, 0, 0]), tensor(2), tensor([1, 4, 5, 2, 0, 0, 0, 0, 0, 0]), tensor(4))\n",
            "pred:\t ['unk', '!', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['unk', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', \"t'en\", 'dois', 'une', '.', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', \"t'en\", 'dois', 'une', '.', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['est-il', 'unk', '?', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['est-il', 'unk', '?', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['je', 'suis', 'unk', '.', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['je', 'suis', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad']\n",
            "\n",
            "pred:\t ['demande', 'à', 'unk', 'qui', '!', 'pad', 'pad', 'pad']\n",
            "\n",
            "tgt:\t ['demande', 'à', 'unk', 'qui', '!', 'eos', 'pad', 'pad', 'pad']\n",
            "\n",
            "Prediction Acc.: 0.9926\n"
          ]
        }
      ],
      "source": [
        "def comp_acc(pred, gt, valid_len):\n",
        "  N, T_gt = gt.shape[:2]\n",
        "  _, T_pr = pred.shape[:2]\n",
        "  assert T_gt == T_pr, 'Prediction and target should have the same length.'\n",
        "  len_mask = torch.arange(T_gt).expand(N, T_gt)\n",
        "  len_mask = len_mask < valid_len[:, None]\n",
        "\n",
        "  pred_crr = (pred == gt).float() * len_mask.float() # filter out the 'bos' token\n",
        "  pred_acc = pred_crr.sum(dim=-1) / (valid_len - 1).float() # minus the 'bos' token\n",
        "  return pred_acc\n",
        "\n",
        "def evaluate_transformer(net, train_iter, device):\n",
        "  net.eval()\n",
        "  acc_list = []\n",
        "  for i, train_data in enumerate(train_iter):\n",
        "    train_data = [ds.to(device) for ds in train_data]\n",
        "\n",
        "    pred = net.predict(*train_data)\n",
        "\n",
        "    pred_acc = comp_acc(pred.detach().cpu(), train_data[2].detach().cpu()[:, 1:], train_data[3].cpu())\n",
        "    acc_list.append(pred_acc)\n",
        "    if i < 5:# print 5 samples from 5 batches\n",
        "      pred = pred[0].detach().cpu()\n",
        "      pred_seq = []\n",
        "      for t in range(MAX_LEN+1):\n",
        "        pred_wd = vocab_fra.index2word[pred[t].item()]\n",
        "        if pred_wd != 'eos':\n",
        "          pred_seq.append(pred_wd)\n",
        "\n",
        "      print('pred:\\t {}\\n'.format(pred_seq))\n",
        "      print('tgt:\\t {}\\n'.format([vocab_fra.index2word[t.item()] for t in train_data[2][0][1:].cpu()]))\n",
        "\n",
        "  print('Prediction Acc.: {:.4f}'.format(torch.cat(acc_list).mean()))\n",
        "\n",
        "seed(1)\n",
        "batch_size = 32\n",
        "\n",
        "vocab_eng, vocab_fra, train_iter = load_data_nmt(batch_size)\n",
        "\n",
        "evaluate_transformer(transformer_net, train_iter, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNSk0hf4FPKW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}